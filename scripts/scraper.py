#!/usr/bin/env python3
"""
K-Rank Beauty Scraper
ì˜¬ë¦¬ë¸Œì˜ ë² ìŠ¤íŠ¸ ì œí’ˆ ë­í‚¹ì„ í¬ë¡¤ë§í•˜ê³  Firebaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import random
import time
import re
from datetime import datetime
from typing import List, Dict, Any
import json

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from dotenv import load_dotenv
from hangul_romanize import Transliter
from hangul_romanize.rule import academic

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ì˜
CATEGORY_MAPPING = {
    'all': {'url_param': None, 'firestore_category': 'beauty'},
    'skincare': {'url_param': '10000010001', 'firestore_category': 'beauty-skincare'},
    'suncare': {'url_param': '10000010011', 'firestore_category': 'beauty-suncare'},
    'masks': {'url_param': '10000010009', 'firestore_category': 'beauty-masks'},
    'makeup': {'url_param': '10000010002', 'firestore_category': 'beauty-makeup'},
    'haircare': {'url_param': '10000010004', 'firestore_category': 'beauty-haircare'},
    'bodycare': {'url_param': '10000010003', 'firestore_category': 'beauty-bodycare'},
}

# ë¸Œëœë“œëª… ì˜ì–´ ë§¤í•‘
BRAND_NAME_MAPPING = {
    # ì£¼ìš” ë¸Œëœë“œ
    'ë©”ë””íë¸Œ': 'Medicube',
    'ì—ìŠ¤ë„¤ì´ì²˜': 'S.Nature',
    'ì—ìŠ¤íŠ¸ë¼': 'AESTURA',
    'ì´ì¦ˆì•¤íŠ¸ë¦¬': 'Isntree',
    'ì›°ë¼ì¥¬': 'Wellage',
    'ë‹¬ë°”': "d'Alba",
    'ë©”ë””í': 'Mediheal',
    'ì„¤í™”ìˆ˜': 'Sulwhasoo',
    'ë¼ë¡œìŠˆí¬ì œ': 'La Roche-Posay',
    'í† ë¦¬ë“ ': 'Torriden',
    'ì•„ëˆ„ì•„': 'Anua',
    'ì°¨ì•¤ë°•': 'CHARMZONE',
    'ë¸”ë‘ë„¤ì´ì²˜': 'BLANC NATURE',
    'í”„ë¦¬ë©”ë¼': 'Primera',
    'í•œìœ¨': 'Hanyul',
    'ì—ì´í”„ë¦´ìŠ¤í‚¨': 'April Skin',
    'ë§ˆë…€ê³µì¥': "Ma:nyo",
    'í—¤ë¼': 'HERA',
    'ENHYPEN': 'ENHYPEN',
    'ìŠ¤í‚¨í‘¸ë“œ': 'SKINFOOD',
    'ë©”ë…¸í‚¨': 'Menoquin',
    'ì˜ë‚´ì¶”ëŸ´': 'So Natural',
    'í¬ëŸ°í‹´': 'Crunchteen',
    'êµ¬ë‹¬': 'GOODAL',
    'ë‹¥í„°ì§€': 'Dr.G',
    'ì •ìƒ˜ë¬¼': 'JUNG SAEM MOOL',
    'í´ë¦¬ì˜¤': 'CLIO',
    'ë¡¬ì•¤': 'rom&nd',
    'í˜ë¦¬í˜ë¼': 'peripera',
    'ì–´ë…¸ë¸Œ': 'UNOVE',
    'ë‹¥í„°ê·¸ë£¨íŠ¸': 'Dr. GROOT',
    'ë¯¸ìŸì„¼': 'MISE EN SCENE',
    'ì¼ë¦¬ìœ¤': 'illiyoon',
    'ì„¸íƒ€í•„': 'Cetaphil',
    
    # ê¸€ë¡œë²Œ ë¸Œëœë“œ (ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš°ë„ í¬í•¨)
    'ë¼ë¡œìŠˆí¬ì œ': 'La Roche-Posay',
    
    # ì¶”ê°€ ë¸Œëœë“œ (í•„ìš”ì‹œ ê³„ì† í™•ì¥)
}



# Firebase ì´ˆê¸°í™”
def initialize_firebase():
    """Firebase Admin SDK ì´ˆê¸°í™”"""
    if not firebase_admin._apps:
        # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì™€ ìƒê´€ì—†ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ serviceAccountKey.json ì‚¬ìš©
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        key_path = os.path.join(project_root, 'serviceAccountKey.json')
        
        cred = credentials.Certificate(key_path)
        firebase_admin.initialize_app(cred)
    return firestore.client()

# Gemini API ì´ˆê¸°í™”
def initialize_gemini():
    """Gemini API ì´ˆê¸°í™”"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in .env file")
    genai.configure(api_key=api_key)
    # models/gemini-2.0-flash: ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ì´ë©° í• ë‹¹ëŸ‰ì´ ì•ˆì •ì ì„
    return genai.GenerativeModel('models/gemini-2.0-flash')




def scrape_olive_young_by_category(category_code: str = None, max_items: int = 20, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    ì˜¬ë¦¬ë¸Œì˜ ì¹´í…Œê³ ë¦¬ë³„ ë² ìŠ¤íŠ¸ ì œí’ˆ í¬ë¡¤ë§ (ScraperAPI ì‚¬ìš©)
    
    Args:
        category_code: ì¹´í…Œê³ ë¦¬ ì½”ë“œ (ì˜ˆ: '10000010001' for Skincare, None for All)
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜
        max_retries: ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    # ScraperAPI í‚¤ í™•ì¸
    scraperapi_key = os.getenv('SCRAPER_API_KEY')
    if not scraperapi_key:
        print("âŒ SCRAPER_API_KEY not found in environment")
        return products
    
    # URL ìƒì„±
    if category_code:
        target_url = f"https://www.oliveyoung.co.kr/store/main/getBestList.do?dispCatNo=900000100100001&fltDispCatNo={category_code}&rowsPerPage=100"
    else:
        target_url = "https://www.oliveyoung.co.kr/store/main/getBestList.do?dispCatNo=900000100100001&rowsPerPage=100"
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸŒ ScraperAPIë¡œ í˜ì´ì§€ ìš”ì²­ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
            print(f"ğŸ“„ URL: {target_url}")
            
            # ScraperAPI íŒŒë¼ë¯¸í„° (render=falseë¡œ ì„¤ì •í•˜ì—¬ JavaScript ì‹¤í–‰ ì „ HTML ê°€ì ¸ì˜¤ê¸°)
            params = {
                'api_key': scraperapi_key,
                'url': target_url,
                'country_code': 'kr',  # í•œêµ­ IP ì‚¬ìš©
                'render': 'false'  # JavaScript ë Œë”ë§ í•˜ì§€ ì•ŠìŒ
            }
            
            # ìš”ì²­ ì „ì†¡
            response = requests.get('http://api.scraperapi.com', params=params, timeout=60)
            
            if response.status_code == 200:
                print("âœ… ScraperAPI ìš”ì²­ ì„±ê³µ!")
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Cloudflare ì²´í¬
                page_title = soup.title.string if soup.title else "No Title"
                if "ì ì‹œë§Œ" in page_title or "Just a moment" in page_title:
                    print(f"âš ï¸  ì—¬ì „íˆ Cloudflare í˜ì´ì§€ ê°ì§€ë¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        time.sleep(5)
                        continue
                    else:
                        print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return products
                
                print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {page_title}")
                
                # ì œí’ˆ íŒŒì‹±
                items = soup.select('div.prd_info')[:max_items]
                
                if len(items) == 0:
                    print("âš ï¸  'div.prd_info'ë¡œ ì œí’ˆì„ ì°¾ì§€ ëª»í•¨")
                    items = soup.select('ul.common_prd_list li')[:max_items]
                
                print(f"âœ… {len(items)}ê°œ ì œí’ˆ ë°œê²¬")
                
                if len(items) == 0:
                    print(f"âš ï¸  ì œí’ˆì„ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        time.sleep(5)
                        continue
                    else:
                        print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return products
                
                # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
                for idx, item in enumerate(items, 1):
                    try:
                        name_elem = item.select_one('.prd_name .tx_name') or item.select_one('p.tx_name')
                        name = name_elem.get_text(strip=True) if name_elem else f"Product {idx}"
                        
                        brand_elem = item.select_one('.tx_brand')
                        brand = brand_elem.get_text(strip=True) if brand_elem else "Unknown"
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë‹¤ì–‘í•œ ì†ì„± í™•ì¸, data-original ìš°ì„ )
                        img_elem = item.select_one('img')
                        image_url = ''
                        if img_elem:
                            # ì—¬ëŸ¬ ì†ì„±ì—ì„œ ì´ë¯¸ì§€ URL ì°¾ê¸° (ìš°ì„ ìˆœìœ„: data-original > data-ref > data-src > src)
                            image_url = (
                                img_elem.get('data-original', '') or 
                                img_elem.get('data-ref', '') or 
                                img_elem.get('data-src', '') or 
                                img_elem.get('src', '')
                            )
                        
                        # placeholder ì´ë¯¸ì§€ í•„í„°ë§
                        if image_url and ('noimg' in image_url or 'placeholder' in image_url or 'loading' in image_url):
                            image_url = ''
                        
                        # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
                        if image_url and not image_url.startswith('http'):
                            image_url = 'https:' + image_url if image_url.startswith('//') else 'https://www.oliveyoung.co.kr' + image_url
                        
                        price_elem = item.select_one('.tx_cur .tx_num')
                        price = price_elem.get_text(strip=True) if price_elem else "0"
                        if price:
                            price = price + "ì›"
                        
                        link_elem = item.select_one('a')
                        buy_url = link_elem.get('href', '') if link_elem else ''
                        if buy_url and not buy_url.startswith('http'):
                            buy_url = 'https://www.oliveyoung.co.kr' + buy_url
                        
                        product = {
                            'rank': idx,
                            'productName': name,
                            'brand': brand,
                            'imageUrl': image_url or "https://images.unsplash.com/photo-1620916566398-39f1143ab7be?w=100&h=100&fit=crop",
                            'price': price,
                            'buyUrl': buy_url,
                            'tags': [],
                            'subcategory': 'skincare',
                            'trend': 0
                        }
                        
                        products.append(product)
                        print(f"  {idx}. {brand} - {name} ({price})")
                        if image_url:
                            print(f"      Image: {image_url[:80]}...")
                        
                    except Exception as e:
                        print(f"âš ï¸  ì œí’ˆ {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                print("âœ… ì œí’ˆ í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
            else:
                print(f"âŒ ScraperAPI ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                    time.sleep(5)
                    continue
                else:
                    print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    
        except requests.exceptions.Timeout:
            print(f"â±ï¸  ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                time.sleep(5)
                continue
            else:
                print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                time.sleep(5)
                continue
            else:
                print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                import traceback
                traceback.print_exc()
    
    return products
async def calculate_trends(db, category_key: str, current_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì´ì „ ë‚ ì§œ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ íŠ¸ë Œë“œ ê³„ì‚°
    
    NOTE: ì´ í•¨ìˆ˜ëŠ” ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ í›„ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤!
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤
        current_products: í˜„ì¬ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´ ë²ˆì—­ ì™„ë£Œëœ ìƒíƒœ)
        
    Returns:
        íŠ¸ë Œë“œê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    from datetime import timedelta
    
    try:
        # ì–´ì œ ë‚ ì§œ (UTC)
        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
        doc_id = f"{yesterday}_{firestore_category}"
        
        print(f"\nğŸ“Š íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday}, ì¹´í…Œê³ ë¦¬: {firestore_category})")
        
        # ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            # ì–´ì œ ë°ì´í„° ì—†ìœ¼ë©´ íŠ¸ë Œë“œ 0
            for product in current_products:
                product['trend'] = 0
            return current_products
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        # ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ìˆœìœ„ ë³€ë™ ê³„ì‚°
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current_item in current_products:
            current_rank = current_item['rank']
            product_name = current_item['productName']
            brand = current_item.get('brand', '')
            
            # 1ì°¨: ì œí’ˆëª…ìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item.get('productName') == product_name:
                    yesterday_rank = old_item.get('rank')
                    break
            
            # 2ì°¨: ì œí’ˆëª…ì´ ë§¤ì¹­ ì•ˆë˜ë©´ ë¸Œëœë“œ + ìˆœìœ„ ë²”ìœ„ë¡œ ë³´ì¡° ë§¤ì¹­
            if yesterday_rank is None and brand:
                for old_item in yesterday_items:
                    # ë¸Œëœë“œê°€ ê°™ê³  ìˆœìœ„ ì°¨ì´ê°€ Â±3 ì´ë‚´
                    if (old_item.get('brand') == brand and 
                        abs(old_item.get('rank', 999) - current_rank) <= 3):
                        # ì œí’ˆëª… ì¼ë¶€ ìœ ì‚¬ì„± ì²´í¬ (ê°„ë‹¨í•œ ë‹¨ì–´ ë§¤ì¹­)
                        old_name_words = set(old_item.get('productName', '').lower().split())
                        new_name_words = set(product_name.lower().split())
                        common_words = old_name_words & new_name_words
                        if len(common_words) >= 2:  # 2ê°œ ì´ìƒ ë‹¨ì–´ ì¼ì¹˜
                            yesterday_rank = old_item.get('rank')
                            print(f"  ğŸ” ë³´ì¡° ë§¤ì¹­: {product_name[:30]}... (rank {current_rank} â‰ˆ {yesterday_rank})")
                            break
            
            if yesterday_rank:
                # íŠ¸ë Œë“œ = ì–´ì œ ìˆœìœ„ - ì˜¤ëŠ˜ ìˆœìœ„ (ì–‘ìˆ˜ë©´ ìƒìŠ¹)
                trend = yesterday_rank - current_rank
                current_item['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {product_name[:40]}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                # ì‹ ê·œ ì§„ì…
                current_item['trend'] = 0
                trend_changes.append(f"  {product_name[:40]}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
        if trend_changes:
            print("ğŸ“ˆ íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
        
        return current_products
        
    except Exception as e:
        print(f"âš ï¸  íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •
        for product in current_products:
            product['trend'] = 0
        return current_products
def auto_romanize_korean(text: str) -> str:
    """
    í•œê¸€ì„ ë¡œë§ˆìë¡œ ìë™ ë³€í™˜
    
    Args:
        text: í•œê¸€ ë˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸
        
    Returns:
        ë¡œë§ˆì ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì´ë¯¸ ì˜ì–´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
    """
    try:
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        has_korean = any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in text)
        
        if has_korean:
            # Transliter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            transliter = Transliter(academic)
            # í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜
            romanized = transliter.translit(text)
            # ê° ë‹¨ì–´ì˜ ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ (Title Case)
            return romanized.title()
        else:
            # ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    except Exception as e:
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        print(f"âš ï¸  Romanization ì˜¤ë¥˜ ({text}): {e}")
        return text


def normalize_product_name(name: str) -> str:
    """
    ì œí’ˆëª…ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    
    Args:
        name: ì›ë³¸ ì œí’ˆëª…
        
    Returns:
        ì •ê·œí™”ëœ ì œí’ˆëª…
    """
    # [ê¸°íš], [ë‹¨í’ˆ], (ì¦ì •) ë“± ì œê±°
    name = re.sub(r'\[.*?\]', '', name)
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì¼ë¶€ë§Œ)
    name = re.sub(r'\([^)]*ê¸°íš[^)]*\)', '', name)
    name = re.sub(r'\([^)]*ì¦ì •[^)]*\)', '', name)
    # +ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì œê±°
    name = re.sub(r'\+.*$', '', name)
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    name = re.sub(r'\s+', ' ', name)
    
    return name.strip()

def translate_brand_names(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë³€í™˜ (ë§¤í•‘ + ìë™ romanization í•˜ì´ë¸Œë¦¬ë“œ)
    
    Args:
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë¸Œëœë“œëª…ì´ ì˜ì–´ë¡œ ë³€í™˜ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ ë¸Œëœë“œëª… ì˜ì–´ ë³€í™˜ ë° ì œí’ˆëª… ì •ê·œí™” ì¤‘...")
    
    new_brands = {}
    
    for product in products:
        korean_brand = product['brand'].strip()
        
        # 1. ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì˜ì–´ ë¸Œëœë“œëª… ì°¾ê¸° (ìš°ì„ ìˆœìœ„)
        if korean_brand in BRAND_NAME_MAPPING:
            product['brand'] = BRAND_NAME_MAPPING[korean_brand]
        else:
            # 2. ìë™ romanization
            romanized = auto_romanize_korean(korean_brand)
            product['brand'] = romanized
            new_brands[korean_brand] = romanized
        
        # ì œí’ˆëª… ì •ê·œí™” (ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°)
        product['productName'] = normalize_product_name(product['productName'])
    
    # ìƒˆë¡œìš´ ë¸Œëœë“œ ë¡œê¹… (ìë™ ë³€í™˜ëœ ë¸Œëœë“œ)
    if new_brands:
        print(f"ğŸ†• ìƒˆë¡œìš´ ë¸Œëœë“œ ìë™ ë³€í™˜ ({len(new_brands)}ê°œ):")
        for korean, english in list(new_brands.items())[:5]:
            print(f"   - {korean} â†’ {english}")
        if len(new_brands) > 5:
            print(f"   ... ì™¸ {len(new_brands) - 5}ê°œ")
    
    print("âœ… ë¸Œëœë“œëª… ë³€í™˜ ì™„ë£Œ")
    
    return products

# ì´ì „ translate_to_english í•¨ìˆ˜ëŠ” ìœ„ì˜ translate_brand_namesë¡œ ëŒ€ì²´ë¨

async def translate_product_names_batch(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆëª…ì„ ì¼ê´„ ë²ˆì—­ (Batch Processing)
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ Gemini AIë¡œ ì œí’ˆëª… ì¼ê´„ ë²ˆì—­ ì¤‘...")
    
    # ì œí’ˆëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    product_names = [f"{p['rank']}. {p['productName']}" for p in products]
    
    prompt = f"""
Translate the following Korean beauty product names into English.
Keep brand names as they are (already in English).
Focus on translating the product description/name part accurately.
Use professional beauty industry terminology.

Product Names:
{chr(10).join(product_names)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "productName": "English Product Name"}},
    {{"rank": 2, "productName": "English Product Name"}},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ì ìš©
        translated_count = 0
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            product_name = trans.get('productName')
            
            for product in products:
                if product['rank'] == rank:
                    product['productName'] = product_name
                    translated_count += 1
                    break
        
        print(f"âœ… ì œí’ˆëª… ë²ˆì—­ ì™„ë£Œ ({translated_count}/{len(products)}ê°œ)")
        
    except Exception as e:
        print(f"âš ï¸ Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ í´ë°±: ìë™ ë¡œë§ˆì ë³€í™˜(Romanization) ì‹œë„")
        # AI ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë¡œë§ˆì ë³€í™˜ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í•œê¸€ ë…¸ì¶œ ë°©ì§€
        for product in products:
            if any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in product['productName']):
                product['productName'] = auto_romanize_korean(product['productName'])
    
    return products


async def generate_tags(model, products: List[Dict[str, Any]], category: str = 'all') -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆë³„ íƒœê·¸ ìë™ ìƒì„±
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        category: ì¹´í…Œê³ ë¦¬
        
    Returns:
        íƒœê·¸ê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸ·ï¸  Gemini AIë¡œ ì œí’ˆ íƒœê·¸ ìë™ ìƒì„± ì¤‘...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ íƒœê·¸ ë§¤í•‘
    category_tags = {
        'all': ['Korean Beauty', 'Best Seller'],
        'skincare': ['Skincare', 'K-Beauty'],
        'suncare': ['Suncare', 'UV Protection'],
        'masks': ['Face Mask', 'Sheet Mask'],
        'makeup': ['Makeup', 'Cosmetics'],
        'haircare': ['Haircare', 'Hair Treatment'],
        'bodycare': ['Bodycare', 'Body Care']
    }
    
    # ì œí’ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì˜ì–´ ë²ˆì—­ëœ ì´ë¦„ ì‚¬ìš©)
    product_info = [f"{p['rank']}. {p['brand']} - {p['productName']}" for p in products]
    
    prompt = f"""
Analyze each beauty product and generate 2-3 unique, relevant tags based on the product's actual characteristics.

IMPORTANT: Each product must have DIFFERENT tags based on its name and brand.
- Identify product type (mask, serum, cream, sunscreen, toner, cleanser, ampoule, essence, etc.)
- Identify key benefits (hydrating, brightening, anti-aging, pore care, soothing, acne care, firming, etc.)
- Identify special features (vegan, dermatologist-tested, sensitive skin, natural ingredients, etc.)

DO NOT use generic tags like "Korean Beauty" or "Best Seller" for all products.
Each product should have unique tags that describe what it actually is.

Examples:
- "Medicube Collagen Jelly Cream" â†’ ["Anti-Aging", "Firming", "Collagen Boost"]
- "Isntree Hyaluronic Acid Toner" â†’ ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]
- "Mediheal Tea Tree Mask Pack 10" â†’ ["Sheet Mask", "Acne Care", "Tea Tree"]
- "AESTURA Atobarrier 365 Cream 80ml" â†’ ["Barrier Cream", "Sensitive Skin", "Moisturizing"]

Products:
{chr(10).join(product_info)}

Response format (JSON):
{{
  "tags": [
    {{"rank": 1, "tags": ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]}},
    {{"rank": 2, "tags": ["Anti-Aging Serum", "Wrinkle Care", "Peptide"]}},
    {{"rank": 3, "tags": ["Sheet Mask", "Brightening", "Vitamin C"]}},
    ...
  ]
}}

JSON only. Make sure each product has DIFFERENT tags that reflect its actual characteristics.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        tag_data = json.loads(result_text)
        
        # ì œí’ˆì— íƒœê·¸ ì ìš©
        for item in tag_data.get('tags', []):
            rank = item.get('rank')
            tags = item.get('tags', [])
            
            for product in products:
                if product['rank'] == rank:
                    product['tags'] = tags
                    break
        
        print("âœ… íƒœê·¸ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸  Gemini íƒœê·¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print(f"ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©: {category}")
        
        # Gemini ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©
        default_tags = category_tags.get(category, ['Korean Beauty', 'Trending'])
        for product in products:
            product['tags'] = default_tags.copy()
    
    return products

async def scrape_netflix(media_type: str = 'tv', max_items: int = 10, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    Netflix Top 10 South Korea TV Shows/Films í¬ë¡¤ë§
    
    Args:
        media_type: 'tv' ë˜ëŠ” 'film'
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸ 10ê°œ)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    for attempt in range(max_retries):
        try:
            async with async_playwright() as p:
                print(f"ğŸ¬ Netflix Top 10 í¬ë¡¤ë§ ì‹œì‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )
                
                page = await context.new_page()
                
                # Netflix Top 10 URL (tv ë˜ëŠ” films)
                url = f"https://top10.netflix.com/south-korea/{media_type}"
                print(f"ğŸ“„ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                
                await page.goto(url, wait_until='networkidle', timeout=60000)
                
                # í…Œì´ë¸”ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    await page.wait_for_selector("table tbody tr", timeout=30000)
                except:
                    print("âš ï¸ í…Œì´ë¸” ì…€ë ‰í„° ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
                
                await page.wait_for_timeout(3000)  # ì¶”ê°€ ë Œë”ë§ ëŒ€ê¸°
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # í…Œì´ë¸” í–‰(Row) ì„ íƒ
                rows = soup.select("table tbody tr")[:max_items]
                print(f"âœ… {len(rows)}ê°œ íƒ€ì´í‹€ ë°œê²¬!")
                
                if len(rows) == 0:
                    print(f"âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    await browser.close()
                    if attempt < max_retries - 1:
                        await asyncio.sleep(5)
                        continue
                    else:
                        return products
                
                for i, row in enumerate(rows, 1):
                    try:
                        # ë¸Œë¼ìš°ì € ë¶„ì„ ê¸°ë°˜ ì…€ë ‰í„°
                        rank_el = row.select_one("span.rank")
                        title_el = row.select_one("td.title button")
                        weeks_el = row.select_one("td[data-uia='top10-table-row-weeks']")
                        img_el = row.select_one("td.title img.desktop-only")
                        
                        rank_text = rank_el.get_text(strip=True) if rank_el else str(i)
                        title = title_el.get_text(strip=True) if title_el else f"Unknown Title {i}"
                        weeks = weeks_el.get_text(strip=True) if weeks_el else "1"
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = img_el.get('src', '') if img_el else 'https://assets.nflxext.com/us/ffe/siteui/common/icons/nficon2016.png'
                        
                        # YouTube íŠ¸ë ˆì¼ëŸ¬ ë§í¬ ìƒì„±
                        trailer_query = f"{title} trailer"
                        trailer_link = f"https://www.youtube.com/results?search_query={trailer_query.replace(' ', '+')}"
                        
                        # media_typeì— ë”°ë¼ type ì„¤ì •
                        item_type = 'TV Show' if media_type == 'tv' else 'Film'
                        default_tag = 'K-Drama' if media_type == 'tv' else 'Korean Film'
                        
                        item = {
                            'rank': int(rank_text) if rank_text.isdigit() else i,
                            'titleEn': title,
                            'titleKo': title,  # ì´í›„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸
                            'imageUrl': image_url,
                            'weeksInTop10': weeks,
                            'type': item_type,
                            'trailerLink': trailer_link,
                            'vpnLink': 'https://nordvpn.com/ko/',
                            'tags': [f"{weeks} Weeks in Top 10", default_tag],
                            'trend': 0
                        }
                        
                        products.append(item)
                        print(f"  {rank_text}ìœ„. {title} ({weeks}ì£¼ ì—°ì† Top 10)")
                        
                    except Exception as e:
                        print(f"âš ï¸ {i}ìœ„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                await browser.close()
                print("âœ… Netflix í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            else:
                import traceback
                traceback.print_exc()
    
    return products

async def translate_media_titles(model, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª©(Netflix)ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    """
    print("\nğŸŒ Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª© í•œêµ­ì–´ ë²ˆì—­ ì¤‘...")
    
    # ì œëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    titles = [f"{item['rank']}. {item['titleEn']}" for item in items]
    
    prompt = f"""
Translate the following Netflix TV Show/Film titles into their official Korean titles.
Some are already Korean dramas, so find their original Korean titles (e.g., 'Squid Game' -> 'ì˜¤ì§•ì–´ ê²Œì„').
Exclude rank numbers from the translation.

Titles:
{chr(10).join(titles)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    {{"rank": 2, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ì ìš©
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            title_ko = trans.get('titleKo')
            
            for item in items:
                if item['rank'] == rank:
                    item['titleKo'] = title_ko
                    break
        
        print("âœ… ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
    
    return items

async def calculate_media_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë¯¸ë””ì–´ ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_media"
        
        print(f"\nğŸ“Š Media íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday})")
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ Media ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            for item in current_items:
                item['trend'] = 0
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ Media ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current in current_items:
            title_en = current.get('titleEn', '')
            title_ko = current.get('titleKo', '')
            current_rank = current['rank']
            
            # ì˜ì–´ ì œëª© ë˜ëŠ” í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if (old_item.get('titleEn') == title_en or 
                    old_item.get('titleKo') == title_ko):
                    yesterday_rank = old_item.get('rank')
                    break
            
            if yesterday_rank:
                trend = yesterday_rank - current_rank
                current['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {title_ko or title_en}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                current['trend'] = 0
                trend_changes.append(f"  {title_ko or title_en}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥
        if trend_changes:
            print("ğŸ“ˆ Media íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        for item in current_items:
            item['trend'] = 0
        return current_items


def save_to_firebase(db, category_key: str, products: List[Dict[str, Any]]):
    """
    Firebase Firestoreì— ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì €ì¥
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤ (ì˜ˆ: 'all', 'skincare', 'suncare')
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ’¾ Firebaseì— {category_key} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì¤‘...")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ (UTC)
    today = datetime.utcnow().strftime('%Y-%m-%d')
    
    # Firestore ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
    firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
    
    # ë¬¸ì„œ ID: {ë‚ ì§œ}_{ì¹´í…Œê³ ë¦¬}
    doc_id = f"{today}_{firestore_category}"
    doc_ref = db.collection('daily_rankings').document(doc_id)
    
    # ë°ì´í„° êµ¬ì¡°
    data = {
        'date': today,
        'category': firestore_category,
        'items': products,
        'updatedAt': firestore.SERVER_TIMESTAMP
    }
    
    # ì €ì¥
    doc_ref.set(data)
    
    print(f"âœ… {len(products)}ê°œ ì œí’ˆì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
    print(f"ğŸ“ ì»¬ë ‰ì…˜: daily_rankings")
    print(f"ğŸ“„ ë¬¸ì„œ ID: {doc_id}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· K-Rank Scraper - Beauty & Media")
    print("=" * 60)
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì í™•ì¸
    run_mode = sys.argv[1] if len(sys.argv) > 1 else "all"  # "beauty", "media", "all"
    
    try:
        # 1. Firebase ì´ˆê¸°í™”
        print("\nğŸ“± Firebase ì´ˆê¸°í™” ì¤‘...")
        db = initialize_firebase()
        print("âœ… Firebase ì—°ê²° ì™„ë£Œ")
        
        # 2. Gemini ì´ˆê¸°í™”
        print("\nğŸ¤– Gemini AI ì´ˆê¸°í™” ì¤‘...")
        model = initialize_gemini()
        print("âœ… Gemini AI ì—°ê²° ì™„ë£Œ")
        
        total_products = 0
        
        # 3. Beauty ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["beauty", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ’„ BEAUTY ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§")
            print("=" * 60)
            
            for category_key, config in CATEGORY_MAPPING.items():
                print("\n" + "-" * 60)
                print(f"ğŸ“¦ {category_key.upper()} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
                print("-" * 60)
                
                # ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
                products = scrape_olive_young_by_category(
                    category_code=config['url_param'],
                    max_items=100  # 100ê°œë¡œ ì¦ê°€
                )
                
                if not products:
                    print(f"âš ï¸  {category_key} ì¹´í…Œê³ ë¦¬ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ë¸Œëœë“œëª… ì˜ì–´ ë³€í™˜ (ë¨¼ì € ì‹¤í–‰)
                products = translate_brand_names(products)
                
                # ì œí’ˆëª… ì˜ì–´ ë²ˆì—­ (Batch Processing)
                products = await translate_product_names_batch(model, products)
                
                # íŠ¸ë Œë“œ ê³„ì‚° (ë²ˆì—­ í›„ ì‹¤í–‰í•˜ì—¬ ì˜ì–´ ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­)
                products = await calculate_trends(db, category_key, products)
                
                # íƒœê·¸ ìë™ ìƒì„±
                products = await generate_tags(model, products, category_key)
                
                # Firebaseì— ì €ì¥
                save_to_firebase(db, category_key, products)
                total_products += len(products)
        
        # 4. Media ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["media", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ¬ MEDIA ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (Netflix)")
            print("=" * 60)
            
            all_media_items = []
            
            # Netflix TV Shows Top 10 í¬ë¡¤ë§
            print("\nğŸ“º Netflix TV Shows í¬ë¡¤ë§ ì¤‘...")
            tv_items = await scrape_netflix(media_type='tv', max_items=10)
            if tv_items:
                all_media_items.extend(tv_items)
                print(f"âœ… TV Shows {len(tv_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ TV Shows ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # Netflix Films Top 10 í¬ë¡¤ë§
            print("\nğŸ¬ Netflix Films í¬ë¡¤ë§ ì¤‘...")
            film_items = await scrape_netflix(media_type='films', max_items=10)
            if film_items:
                all_media_items.extend(film_items)
                print(f"âœ… Films {len(film_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ Films ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            if all_media_items:
                # íŠ¸ë Œë“œ ê³„ì‚°
                all_media_items = await calculate_media_trends(db, all_media_items)
                
                # í•œêµ­ì–´ ì œëª© ë²ˆì—­
                all_media_items = await translate_media_titles(model, all_media_items)
                
                # Media ì €ì¥ ë¡œì§
                today = datetime.utcnow().strftime('%Y-%m-%d')
                doc_id = f"{today}_media"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'media',
                    'items': all_media_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                doc_ref.set(data)
                print(f"âœ… {len(all_media_items)}ê°œ íƒ€ì´í‹€ì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                print(f"   - TV Shows: {len(tv_items)}ê°œ")
                print(f"   - Films: {len(film_items)}ê°œ")
                total_products += len(all_media_items)
            else:
                print("âš ï¸ Netflixì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"  - ì´ ì•„ì´í…œ ìˆ˜: {total_products}ê°œ")
        print(f"  - ì‹¤í–‰ ëª¨ë“œ: {run_mode.upper()}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # ì‚¬ìš©ë²•:
    # python scraper.py           # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‹¤í–‰
    # python scraper.py beauty    # Beautyë§Œ ì‹¤í–‰
    # python scraper.py media     # Mediaë§Œ ì‹¤í–‰
    asyncio.run(main())
