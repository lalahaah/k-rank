#!/usr/bin/env python3
"""
K-Rank Beauty Scraper
ì˜¬ë¦¬ë¸Œì˜ ë² ìŠ¤íŠ¸ ì œí’ˆ ë­í‚¹ì„ í¬ë¡¤ë§í•˜ê³  Firebaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import random
import time
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
import json
import math

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from dotenv import load_dotenv
from hangul_romanize import Transliter
from hangul_romanize.rule import academic

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
env_path = os.path.join(project_root, '.env')
load_dotenv(env_path)

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ì˜ (ë¬´ì‹ ì‚¬ ë·°í‹° ê¸°ì¤€)
CATEGORY_MAPPING = {
    'all': {'url_param': '', 'firestore_category': 'beauty'},
    'skincare': {'url_param': '104001', 'firestore_category': 'beauty-skincare'},
    'suncare': {'url_param': '104002', 'firestore_category': 'beauty-suncare'},
    'masks': {'url_param': '104013', 'firestore_category': 'beauty-masks'},
    'makeup': {'url_param': '104014', 'firestore_category': 'beauty-makeup'},
    'haircare': {'url_param': '104006', 'firestore_category': 'beauty-haircare'},
    'bodycare': {'url_param': '104007', 'firestore_category': 'beauty-bodycare'},
}

# ë¸Œëœë“œëª… ì˜ì–´ ë§¤í•‘
BRAND_NAME_MAPPING = {
    # ì£¼ìš” ë¸Œëœë“œ
    'ë©”ë””íë¸Œ': 'Medicube',
    'ì—ìŠ¤ë„¤ì´ì²˜': 'S.Nature',
    'ì—ìŠ¤íŠ¸ë¼': 'AESTURA',
    'ì´ì¦ˆì•¤íŠ¸ë¦¬': 'Isntree',
    'ì›°ë¼ì¥¬': 'Wellage',
    'ë‹¬ë°”': "d'Alba",
    'ë©”ë””í': 'Mediheal',
    'ì„¤í™”ìˆ˜': 'Sulwhasoo',
    'ë¼ë¡œìŠˆí¬ì œ': 'La Roche-Posay',
    'í† ë¦¬ë“ ': 'Torriden',
    'ì•„ëˆ„ì•„': 'Anua',
    'ì°¨ì•¤ë°•': 'CHARMZONE',
    'ë¸”ë‘ë„¤ì´ì²˜': 'BLANC NATURE',
    'í”„ë¦¬ë©”ë¼': 'Primera',
    'í•œìœ¨': 'Hanyul',
    'ì—ì´í”„ë¦´ìŠ¤í‚¨': 'April Skin',
    'ë§ˆë…€ê³µì¥': "Ma:nyo",
    'í—¤ë¼': 'HERA',
    'ENHYPEN': 'ENHYPEN',
    'ìŠ¤í‚¨í‘¸ë“œ': 'SKINFOOD',
    'ë©”ë…¸í‚¨': 'Menoquin',
    'ì˜ë‚´ì¶”ëŸ´': 'So Natural',
    'í¬ëŸ°í‹´': 'Crunchteen',
    'êµ¬ë‹¬': 'GOODAL',
    'ë‹¥í„°ì§€': 'Dr.G',
    'ì •ìƒ˜ë¬¼': 'JUNG SAEM MOOL',
    'í´ë¦¬ì˜¤': 'CLIO',
    'ë¡¬ì•¤': 'rom&nd',
    'í˜ë¦¬í˜ë¼': 'peripera',
    'ì–´ë…¸ë¸Œ': 'UNOVE',
    'ë‹¥í„°ê·¸ë£¨íŠ¸': 'Dr. GROOT',
    'ë¯¸ìŸì„¼': 'MISE EN SCENE',
    'ì¼ë¦¬ìœ¤': 'illiyoon',
    'ì„¸íƒ€í•„': 'Cetaphil',
    
    # ê¸€ë¡œë²Œ ë¸Œëœë“œ (ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš°ë„ í¬í•¨)
    'ë¼ë¡œìŠˆí¬ì œ': 'La Roche-Posay',
    
    # ì¶”ê°€ ë¸Œëœë“œ (í•„ìš”ì‹œ ê³„ì† í™•ì¥)
}



# Firebase ì´ˆê¸°í™”
def initialize_firebase():
    """Firebase Admin SDK ì´ˆê¸°í™”"""
    if not firebase_admin._apps:
        # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì™€ ìƒê´€ì—†ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ serviceAccountKey.json ì‚¬ìš©
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        key_path = os.path.join(project_root, 'serviceAccountKey.json')
        
        cred = credentials.Certificate(key_path)
        firebase_admin.initialize_app(cred)
    return firestore.client()


# Gemini API ì´ˆê¸°í™”
def initialize_gemini():
    """Gemini API ì´ˆê¸°í™”"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in .env file")
    genai.configure(api_key=api_key)
    # models/gemini-2.0-flash: ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ì´ë©° í• ë‹¹ëŸ‰ì´ ì•ˆì •ì ì„
    return genai.GenerativeModel('models/gemini-2.0-flash')


async def get_amazon_image(query: str) -> str:
    """
    ì•„ë§ˆì¡´ ê²€ìƒ‰ì„ í†µí•´ ì œí’ˆ ì´ë¯¸ì§€ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (WebScraping.ai ì‚¬ìš©)
    """
    api_key = os.getenv('WEBSCRAPING_AI_API_KEY')
    if not api_key:
        return ""
    
    search_url = f"https://www.amazon.com/s?k={query.replace(' ', '+')}"
    
    try:
        params = {
            'api_key': api_key,
            'url': search_url,
            'proxy': 'residential',
            'country': 'us'
        }
        
        response = requests.get('https://api.webscraping.ai/html', params=params, timeout=60)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # ì•„ë§ˆì¡´ ê²€ìƒ‰ ê²°ê³¼ ì´ë¯¸ì§€ ì„ íƒì
            img_elem = soup.select_one('div[data-component-type="s-search-result"] img.s-image')
            if img_elem:
                return img_elem.get('src', '')
    except Exception as e:
        print(f"âš ï¸ Amazon ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
    
    return ""




def scrape_musinsa_beauty_by_category(category_code: str = '', max_items: int = 20, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    ë¬´ì‹ ì‚¬ ë·°í‹° ì¹´í…Œê³ ë¦¬ë³„ ë² ìŠ¤íŠ¸ ì œí’ˆ í¬ë¡¤ë§ (WebScraping.ai ì‚¬ìš©)
    ë²•ì  ì•ˆì „ì„±ì„ ìœ„í•´ ìƒìœ„ 20ê°œ ì œí’ˆë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    products = []
    
    api_key = os.getenv('WEBSCRAPING_AI_API_KEY')
    if not api_key:
        print("âŒ WEBSCRAPING_AI_API_KEY not found in environment")
        return products
    # URL ìƒì„± (ë¬´ì‹ ì‚¬ ë·°í‹° ë­í‚¹ í˜ì´ì§€)
    if category_code:
        target_url = f"https://www.musinsa.com/main/beauty/ranking?categoryCode={category_code}"
    else:
        target_url = "https://www.musinsa.com/main/beauty/ranking"
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸŒ WebScraping.aië¡œ ë¬´ì‹ ì‚¬ í˜ì´ì§€ ìš”ì²­ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
            
            params = {
                'api_key': api_key,
                'url': target_url,
                'proxy': 'residential',
                'country': 'kr',
                'js_render': 'true', 
                'wait_for': '.gtm-select-item'
            }
            
            response = requests.get('https://api.webscraping.ai/html', params=params, timeout=120)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì œí’ˆ íŒŒì‹± (CSS ì„ íƒì ê¸°ë°˜)
                # ë¬´ì‹ ì‚¬ ë·°í‹° ë­í‚¹ì˜ ì¹´ë“œ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                items = soup.select('div[class*="UIProductColumn__Wrap"]')[:max_items]
                
                if len(items) == 0:
                    # ë°±ì—… ì„ íƒì ì‹œë„ (ë¦¬ë‰´ì–¼ ëŒ€ì‘)
                    items = soup.select('div[class*="UIProductColumn"]')[:max_items]
                
                if len(items) == 0:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: gtm í´ë˜ìŠ¤ ì‚¬ìš©
                    items = soup.select('.gtm-select-item')[:max_items]
                
                print(f"âœ… {len(items)}ê°œ ì œí’ˆ ë°œê²¬")
                
                for idx, item in enumerate(items, 1):
                    try:
                        # ë¸Œëœë“œëª…
                        brand_elem = item.select_one('a.gtm-click-brand p') or item.select_one('a[class*="gtm-click-brand"] p')
                        brand = brand_elem.get_text(strip=True) if brand_elem else "Unknown"
                        
                        # ìƒí’ˆëª…
                        name_elem = item.select_one('a.gtm-select-item p') or item.select_one('a[class*="gtm-select-item"] p')
                        name = name_elem.get_text(strip=True) if name_elem else f"Product {idx}"
                        
                        # ê°€ê²© (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ)
                        price_elem = item.select_one('span[class*="UIProductColumn__PriceText"]') or item.select_one('span.text-body_13px_semi')
                        price = price_elem.get_text(strip=True) if price_elem else "0ì›"
                        
                        # ì´ë¯¸ì§€ (ë¬´ì‹ ì‚¬ ì¸ë„¤ì¼ ìˆ˜ì§‘ - ì•„ë§ˆì¡´ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°±ìš©)
                        img_elem = item.select_one('img')
                        musinsa_img = ""
                        if img_elem:
                            musinsa_img = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('lazy-src')
                        
                        product = {
                            'rank': idx,
                            'productName': name,
                            'brand': brand,
                            'imageUrl': musinsa_img or "https://images.unsplash.com/photo-1620916566398-39f1143ab7be?w=100&h=100&fit=crop",
                            'price': price,
                            'buyUrl': f"https://www.amazon.com/s?k={brand}+{name}",
                            'tags': [],
                            'subcategory': 'beauty',
                            'trend': 0,
                            'nikIndex': 0,
                            'culturalContext': ""
                        }
                        
                        products.append(product)
                        
                    except Exception as e:
                        print(f"âš ï¸  ì œí’ˆ {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                if products:
                    print("âœ… ë¬´ì‹ ì‚¬ ì œí’ˆ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
                    break
                    
            else:
                print(f"âŒ WebScraping.ai ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}")
                time.sleep(10)
                    
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            time.sleep(5)
    
    return products
async def calculate_trends(db, category_key: str, current_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì´ì „ ë‚ ì§œ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ íŠ¸ë Œë“œ ê³„ì‚°
    
    NOTE: ì´ í•¨ìˆ˜ëŠ” ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ í›„ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤!
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤
        current_products: í˜„ì¬ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´ ë²ˆì—­ ì™„ë£Œëœ ìƒíƒœ)
        
    Returns:
        íŠ¸ë Œë“œê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    from datetime import timedelta
    
    try:
        # ì–´ì œ ë‚ ì§œ (UTC)
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
        doc_id = f"{yesterday}_{firestore_category}"
        
        print(f"\nğŸ“Š íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday}, ì¹´í…Œê³ ë¦¬: {firestore_category})")
        
        # ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            # ì–´ì œ ë°ì´í„° ì—†ìœ¼ë©´ íŠ¸ë Œë“œ 0
            for product in current_products:
                product['trend'] = 0
            return current_products
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        # ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ìˆœìœ„ ë³€ë™ ê³„ì‚°
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current_item in current_products:
            current_rank = current_item['rank']
            product_name = current_item['productName']
            brand = current_item.get('brand', '')
            
            # 1ì°¨: ì œí’ˆëª…ìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item.get('productName') == product_name:
                    yesterday_rank = old_item.get('rank')
                    break
            
            # 2ì°¨: ì œí’ˆëª…ì´ ë§¤ì¹­ ì•ˆë˜ë©´ ë¸Œëœë“œ + ìˆœìœ„ ë²”ìœ„ë¡œ ë³´ì¡° ë§¤ì¹­
            if yesterday_rank is None and brand:
                for old_item in yesterday_items:
                    # ë¸Œëœë“œê°€ ê°™ê³  ìˆœìœ„ ì°¨ì´ê°€ Â±3 ì´ë‚´
                    if (old_item.get('brand') == brand and 
                        abs(old_item.get('rank', 999) - current_rank) <= 3):
                        # ì œí’ˆëª… ì¼ë¶€ ìœ ì‚¬ì„± ì²´í¬ (ê°„ë‹¨í•œ ë‹¨ì–´ ë§¤ì¹­)
                        old_name_words = set(old_item.get('productName', '').lower().split())
                        new_name_words = set(product_name.lower().split())
                        common_words = old_name_words & new_name_words
                        if len(common_words) >= 2:  # 2ê°œ ì´ìƒ ë‹¨ì–´ ì¼ì¹˜
                            yesterday_rank = old_item.get('rank')
                            print(f"  ğŸ” ë³´ì¡° ë§¤ì¹­: {product_name[:30]}... (rank {current_rank} â‰ˆ {yesterday_rank})")
                            break
            
            if yesterday_rank:
                # íŠ¸ë Œë“œ = ì–´ì œ ìˆœìœ„ - ì˜¤ëŠ˜ ìˆœìœ„ (ì–‘ìˆ˜ë©´ ìƒìŠ¹)
                trend = yesterday_rank - current_rank
                current_item['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {product_name[:40]}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                # ì‹ ê·œ ì§„ì…
                current_item['trend'] = 0
                trend_changes.append(f"  {product_name[:40]}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
        if trend_changes:
            print("ğŸ“ˆ íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
        
        return current_products
        
    except Exception as e:
        print(f"âš ï¸  íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •
        for product in current_products:
            product['trend'] = 0
        return current_products
def auto_romanize_korean(text: str) -> str:
    """
    í•œê¸€ì„ ë¡œë§ˆìë¡œ ìë™ ë³€í™˜
    
    Args:
        text: í•œê¸€ ë˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸
        
    Returns:
        ë¡œë§ˆì ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì´ë¯¸ ì˜ì–´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
    """
    try:
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        has_korean = any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in text)
        
        if has_korean:
            # Transliter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            transliter = Transliter(academic)
            # í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜
            romanized = transliter.translit(text)
            # ê° ë‹¨ì–´ì˜ ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ (Title Case)
            return romanized.title()
        else:
            # ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    except Exception as e:
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        print(f"âš ï¸  Romanization ì˜¤ë¥˜ ({text}): {e}")
        return text


def normalize_product_name(name: str) -> str:
    """
    ì œí’ˆëª…ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    
    Args:
        name: ì›ë³¸ ì œí’ˆëª…
        
    Returns:
        ì •ê·œí™”ëœ ì œí’ˆëª…
    """
    # [ê¸°íš], [ë‹¨í’ˆ], (ì¦ì •) ë“± ì œê±°
    name = re.sub(r'\[.*?\]', '', name)
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì¼ë¶€ë§Œ)
    name = re.sub(r'\([^)]*ê¸°íš[^)]*\)', '', name)
    name = re.sub(r'\([^)]*ì¦ì •[^)]*\)', '', name)
    # +ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì œê±°
    name = re.sub(r'\+.*$', '', name)
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    name = re.sub(r'\s+', ' ', name)
    
    return name.strip()

def translate_brand_names(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë³€í™˜ (ë§¤í•‘ + ìë™ romanization í•˜ì´ë¸Œë¦¬ë“œ)
    
    Args:
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë¸Œëœë“œëª…ì´ ì˜ì–´ë¡œ ë³€í™˜ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ ë¸Œëœë“œëª… ì˜ì–´ ë³€í™˜ ë° ì œí’ˆëª… ì •ê·œí™” ì¤‘...")
    
    new_brands = {}
    
    for product in products:
        korean_brand = product['brand'].strip()
        
        # 1. ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì˜ì–´ ë¸Œëœë“œëª… ì°¾ê¸° (ìš°ì„ ìˆœìœ„)
        if korean_brand in BRAND_NAME_MAPPING:
            product['brand'] = BRAND_NAME_MAPPING[korean_brand]
        else:
            # 2. ìë™ romanization
            romanized = auto_romanize_korean(korean_brand)
            product['brand'] = romanized
            new_brands[korean_brand] = romanized
        
        # ì œí’ˆëª… ì •ê·œí™” (ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°)
        product['productName'] = normalize_product_name(product['productName'])
    
    # ìƒˆë¡œìš´ ë¸Œëœë“œ ë¡œê¹… (ìë™ ë³€í™˜ëœ ë¸Œëœë“œ)
    if new_brands:
        print(f"ğŸ†• ìƒˆë¡œìš´ ë¸Œëœë“œ ìë™ ë³€í™˜ ({len(new_brands)}ê°œ):")
        for korean, english in list(new_brands.items())[:5]:
            print(f"   - {korean} â†’ {english}")
        if len(new_brands) > 5:
            print(f"   ... ì™¸ {len(new_brands) - 5}ê°œ")
    
    print("âœ… ë¸Œëœë“œëª… ë³€í™˜ ì™„ë£Œ")
    
    return products

# ì´ì „ translate_to_english í•¨ìˆ˜ëŠ” ìœ„ì˜ translate_brand_namesë¡œ ëŒ€ì²´ë¨

async def translate_product_names_batch(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆëª…ì„ ì¼ê´„ ë²ˆì—­ (Batch Processing)
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ Gemini AIë¡œ ì œí’ˆëª… ì¼ê´„ ë²ˆì—­ ì¤‘...")
    
    # ì œí’ˆëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    product_names = [f"{p['rank']}. {p['productName']}" for p in products]
    
    prompt = f"""
Translate the following Korean beauty product names into English.
Keep brand names as they are (already in English).
Focus on translating the product description/name part accurately.
Use professional beauty industry terminology.

Additionally, for each product, generate:
1. "nikIndex": A proprietary popularity score from 85.0 to 99.9 based on current K-beauty viral trends.
2. "culturalContext": A very short (max 1 sentence) explanation of why this is trending in Korea (e.g., "Trending on Olive Young for deep hydration", "Viral on TikTok for glass skin finish").
3. "imageQuery": A clean English search term to find this product's image (e.g., "Medicube Zero Pore Pad 2.0").

Product Names:
{chr(10).join(product_names)}

Response format (JSON):
{{
  "translations": [
    {{
      "rank": 1, 
      "productName": "English Product Name", 
      "nikIndex": 98.5, 
      "culturalContext": "Explanation",
      "imageQuery": "Search Term"
    }},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ë° AI ë°ì´í„° ì ìš©
        translated_count = 0
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            product_name = trans.get('productName')
            nik_index = trans.get('nikIndex', 0)
            cultural_context = trans.get('culturalContext', "")
            image_query = trans.get('imageQuery', "")
            
            for product in products:
                if product['rank'] == rank:
                    product['productName'] = product_name
                    product['nikIndex'] = nik_index
                    product['culturalContext'] = cultural_context
                    # image_queryëŠ” ë‚˜ì¤‘ì— ì•„ë§ˆì¡´ ê²€ìƒ‰ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
                    if image_query:
                        product['buyUrl'] = f"https://www.amazon.com/s?k={image_query.replace(' ', '+')}&tag={os.getenv('NEXT_PUBLIC_AMAZON_AFFILIATE_ID', 'krank-20')}"
                    
                    translated_count += 1
                    break
        
        print(f"âœ… ì œí’ˆëª… ë²ˆì—­ ì™„ë£Œ ({translated_count}/{len(products)}ê°œ)")
        
    except Exception as e:
        print(f"âš ï¸ Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ í´ë°±: ìë™ ë¡œë§ˆì ë³€í™˜(Romanization) ì‹œë„")
        # AI ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë¡œë§ˆì ë³€í™˜ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í•œê¸€ ë…¸ì¶œ ë°©ì§€
        for product in products:
            if any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in product['productName']):
                product['productName'] = auto_romanize_korean(product['productName'])
    
    return products


async def generate_tags(model, products: List[Dict[str, Any]], category: str = 'all') -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆë³„ íƒœê·¸ ìë™ ìƒì„±
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        category: ì¹´í…Œê³ ë¦¬
        
    Returns:
        íƒœê·¸ê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸ·ï¸  Gemini AIë¡œ ì œí’ˆ íƒœê·¸ ìë™ ìƒì„± ì¤‘...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ íƒœê·¸ ë§¤í•‘
    category_tags = {
        'all': ['Korean Beauty', 'Best Seller'],
        'skincare': ['Skincare', 'K-Beauty'],
        'suncare': ['Suncare', 'UV Protection'],
        'masks': ['Face Mask', 'Sheet Mask'],
        'makeup': ['Makeup', 'Cosmetics'],
        'haircare': ['Haircare', 'Hair Treatment'],
        'bodycare': ['Bodycare', 'Body Care']
    }
    
    # ì œí’ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì˜ì–´ ë²ˆì—­ëœ ì´ë¦„ ì‚¬ìš©)
    product_info = [f"{p['rank']}. {p['brand']} - {p['productName']}" for p in products]
    
    prompt = f"""
Analyze each beauty product and generate 2-3 unique, relevant tags based on the product's actual characteristics.

IMPORTANT: Each product must have DIFFERENT tags based on its name and brand.
- Identify product type (mask, serum, cream, sunscreen, toner, cleanser, ampoule, essence, etc.)
- Identify key benefits (hydrating, brightening, anti-aging, pore care, soothing, acne care, firming, etc.)
- Identify special features (vegan, dermatologist-tested, sensitive skin, natural ingredients, etc.)

DO NOT use generic tags like "Korean Beauty" or "Best Seller" for all products.
Each product should have unique tags that describe what it actually is.

Examples:
- "Medicube Collagen Jelly Cream" â†’ ["Anti-Aging", "Firming", "Collagen Boost"]
- "Isntree Hyaluronic Acid Toner" â†’ ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]
- "Mediheal Tea Tree Mask Pack 10" â†’ ["Sheet Mask", "Acne Care", "Tea Tree"]
- "AESTURA Atobarrier 365 Cream 80ml" â†’ ["Barrier Cream", "Sensitive Skin", "Moisturizing"]

Products:
{chr(10).join(product_info)}

Response format (JSON):
{{
  "tags": [
    {{"rank": 1, "tags": ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]}},
    {{"rank": 2, "tags": ["Anti-Aging Serum", "Wrinkle Care", "Peptide"]}},
    {{"rank": 3, "tags": ["Sheet Mask", "Brightening", "Vitamin C"]}},
    ...
  ]
}}

JSON only. Make sure each product has DIFFERENT tags that reflect its actual characteristics.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        tag_data = json.loads(result_text)
        
        # ì œí’ˆì— íƒœê·¸ ì ìš©
        for item in tag_data.get('tags', []):
            rank = item.get('rank')
            tags = item.get('tags', [])
            
            for product in products:
                if product['rank'] == rank:
                    product['tags'] = tags
                    break
        
        print("âœ… íƒœê·¸ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸  Gemini íƒœê·¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print(f"ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©: {category}")
        
        # Gemini ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©
        default_tags = category_tags.get(category, ['Korean Beauty', 'Trending'])
        for product in products:
            product['tags'] = default_tags.copy()
    
    return products

async def scrape_netflix(media_type: str = 'tv', max_items: int = 10, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    Netflix Top 10 South Korea TV Shows/Films í¬ë¡¤ë§
    
    Args:
        media_type: 'tv' ë˜ëŠ” 'film'
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸ 10ê°œ)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    for attempt in range(max_retries):
        try:
            async with async_playwright() as p:
                print(f"ğŸ¬ Netflix Top 10 í¬ë¡¤ë§ ì‹œì‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )
                
                page = await context.new_page()
                
                # Netflix Top 10 URL (tv ë˜ëŠ” films)
                url = f"https://top10.netflix.com/south-korea/{media_type}"
                print(f"ğŸ“„ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                
                await page.goto(url, wait_until='networkidle', timeout=60000)
                
                # í…Œì´ë¸”ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    await page.wait_for_selector("table tbody tr", timeout=30000)
                except:
                    print("âš ï¸ í…Œì´ë¸” ì…€ë ‰í„° ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
                
                await page.wait_for_timeout(3000)  # ì¶”ê°€ ë Œë”ë§ ëŒ€ê¸°
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # í…Œì´ë¸” í–‰(Row) ì„ íƒ
                rows = soup.select("table tbody tr")[:max_items]
                print(f"âœ… {len(rows)}ê°œ íƒ€ì´í‹€ ë°œê²¬!")
                
                if len(rows) == 0:
                    print(f"âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    await browser.close()
                    if attempt < max_retries - 1:
                        await asyncio.sleep(5)
                        continue
                    else:
                        return products
                
                for i, row in enumerate(rows, 1):
                    try:
                        # ë¸Œë¼ìš°ì € ë¶„ì„ ê¸°ë°˜ ì…€ë ‰í„°
                        rank_el = row.select_one("span.rank")
                        title_el = row.select_one("td.title button")
                        weeks_el = row.select_one("td[data-uia='top10-table-row-weeks']")
                        img_el = row.select_one("td.title img.desktop-only")
                        
                        rank_text = rank_el.get_text(strip=True) if rank_el else str(i)
                        title = title_el.get_text(strip=True) if title_el else f"Unknown Title {i}"
                        weeks = weeks_el.get_text(strip=True) if weeks_el else "1"
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = img_el.get('src', '') if img_el else 'https://assets.nflxext.com/us/ffe/siteui/common/icons/nficon2016.png'
                        
                        # YouTube íŠ¸ë ˆì¼ëŸ¬ ë§í¬ ìƒì„±
                        trailer_query = f"{title} trailer"
                        trailer_link = f"https://www.youtube.com/results?search_query={trailer_query.replace(' ', '+')}"
                        
                        # media_typeì— ë”°ë¼ type ì„¤ì •
                        item_type = 'TV Show' if media_type == 'tv' else 'Film'
                        default_tag = 'K-Drama' if media_type == 'tv' else 'Korean Film'
                        
                        item = {
                            'rank': int(rank_text) if rank_text.isdigit() else i,
                            'titleEn': title,
                            'titleKo': title,  # ì´í›„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸
                            'imageUrl': image_url,
                            'weeksInTop10': weeks,
                            'type': item_type,
                            'trailerLink': trailer_link,
                            'vpnLink': 'https://nordvpn.com/ko/',
                            'tags': [f"{weeks} Weeks in Top 10", default_tag],
                            'trend': 0
                        }
                        
                        products.append(item)
                        print(f"  {rank_text}ìœ„. {title} ({weeks}ì£¼ ì—°ì† Top 10)")
                        
                    except Exception as e:
                        print(f"âš ï¸ {i}ìœ„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                await browser.close()
                print("âœ… Netflix í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            else:
                import traceback
                traceback.print_exc()
    
    return products

async def translate_media_titles(model, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª©(Netflix)ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    """
    print("\nğŸŒ Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª© í•œêµ­ì–´ ë²ˆì—­ ì¤‘...")
    
    # ì œëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    titles = [f"{item['rank']}. {item['titleEn']}" for item in items]
    
    prompt = f"""
Translate the following Netflix TV Show/Film titles into their official Korean titles.
Some are already Korean dramas, so find their original Korean titles (e.g., 'Squid Game' -> 'ì˜¤ì§•ì–´ ê²Œì„').
Exclude rank numbers from the translation.

Titles:
{chr(10).join(titles)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    {{"rank": 2, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    ...
  ]
}}

JSON only.
"""
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ì ìš©
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            title_ko = trans.get('titleKo')
            
            for item in items:
                if item['rank'] == rank:
                    item['titleKo'] = title_ko
                    break
        
        print(f"âœ… ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸  ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì˜¤ë¥˜: {e}")
        # ì‹¤íŒ¨ ì‹œ ì˜ì–´ ì œëª©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        for item in items:
            item['titleKo'] = item['titleEn']
            
    return items

def get_google_place_stats(name: str, address: str) -> Dict[str, Any]:
    """
    Google Places API (New)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œì˜ ìƒì„¸ í†µê³„(í‰ì , ë¦¬ë·° ìˆ˜)ë¥¼ ê°€ì ¸ì˜´
    """
    api_key = os.getenv('GOOGLE_PLACES_API_KEY') or os.getenv('GOOGLE_MAPS_API_KEY')
    if not api_key:
        return {}
        
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName,places.rating,places.userRatingCount,places.googleMapsUri,places.photos"
    }
    
    # "ì¥ì†Œëª… + ì£¼ì†Œ(ì¼ë¶€)"ë¡œ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ ë†’ì„
    search_query = f"{name} {address.split()[:2]}" 
    data = {
        "textQuery": search_query,
        "languageCode": "en",
        "maxResultCount": 1
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=10)
        if response.status_code == 200:
            places = response.json().get("places", [])
            if places:
                p = places[0]
                return {
                    "rating": p.get("rating", 0),
                    "userRatingCount": p.get("userRatingCount", 0),
                    "googleMapsUri": p.get("googleMapsUri", ""),
                    "photo_name": p.get("photos", [{}])[0].get("name", "") if p.get("photos") else ""
                }
    except Exception as e:
        print(f"âš ï¸ Google Places API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ({name}): {e}")
        
    return {}

async def scrape_tour_api(max_items: int = 50) -> List[Dict[str, Any]]:
    """
    í•œêµ­ê´€ê´‘ê³µì‚¬ TourAPIë¥¼ í†µí•´ ì¸ê¸° ì—¬í–‰ì§€ ì •ë³´ ìˆ˜ì§‘
    """
    api_key = os.getenv('TOUR_API_KEY')
    if not api_key:
        print("âŒ TOUR_API_KEY not found in environment")
        return []

    # areaBasedList2 ì—”ë“œí¬ì¸íŠ¸ (êµ­ë¬¸ ì„œë¹„ìŠ¤ê°€ ë°ì´í„°ê°€ ê°€ì¥ í’ë¶€í•¨)
    base_url = "https://apis.data.go.kr/B551011/KorService2/areaBasedList2"
    
    # ê³µí†µ íŒŒë¼ë¯¸í„° (arrange=B: ì¸ê¸°ìˆœ)
    common_params = f"&numOfRows={max_items}&pageNo=1&MobileOS=ETC&MobileApp=KRank&_type=json&arrange=B"

    content_types = [12, 14, 15]
    all_places = []
    
    for c_type in content_types:
        try:
            print(f"ğŸŒ TourAPI ìš”ì²­ ì¤‘ (contentTypeId: {c_type})...")
            # serviceKeyë¥¼ ì§ì ‘ í¬í•¨í•œ URL ìƒì„± (Double Encoding ë°©ì§€)
            url = f"{base_url}?serviceKey={api_key}{common_params}&contentTypeId={c_type}"
            
            response = requests.get(url, timeout=15)
            if response.status_code == 200:
                try:
                    data = response.json()
                except Exception:
                    print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {response.text[:200]}")
                    continue

                if not isinstance(data, dict):
                    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {data}")
                    continue

                # ì•ˆì „í•˜ê²Œ ì¤‘ì²©ëœ ë°ì´í„° ì¶”ì¶œ
                response_obj = data.get("response", {})
                if not isinstance(response_obj, dict):
                    print(f"âŒ TourAPI ì‘ë‹µ ì˜¤ë¥˜ (responseê°€ dict ì•„ë‹˜): {response_obj}")
                    continue
                
                body = response_obj.get("body", {})
                if not isinstance(body, dict):
                    if "resultCode" in data:
                        print(f"âŒ TourAPI ë¹„ì¦ˆë‹ˆìŠ¤ ì˜¤ë¥˜: {data.get('resultMsg')} (Code: {data.get('resultCode')})")
                    else:
                        print(f"âŒ TourAPI ì‘ë‹µ ì˜¤ë¥˜ (bodyê°€ dict ì•„ë‹˜): {body}")
                    continue
                
                items_obj = body.get("items", {})
                items = []
                if isinstance(items_obj, dict):
                    items = items_obj.get("item", [])
                elif isinstance(items_obj, str) and not items_obj:
                    items = []
                
                if not items:
                    print(f"âš ï¸ contentTypeId {c_type}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                if isinstance(items, dict):
                    items = [items]
                
                for item in items:
                    title = item.get("title")
                    if not title: continue
                    
                    place = {
                        "name_ko": title,
                        "name_en": title, # Gemini ë‹¨ê³„ì—ì„œ ë²ˆì—­ë¨
                        "address_ko": item.get("addr1", ""),
                        "location": item.get("addr1", "").split()[0] if item.get("addr1") else "Unknown",
                        "imageUrl": item.get("firstimage") or "https://images.unsplash.com/photo-1544273677-277914bd9466?w=800&fit=crop",
                        "mapx": item.get("mapx"),
                        "mapy": item.get("mapy"),
                        "content_id": item.get("contentid"),
                        "content_type": c_type,
                        "views": "N/A",  # TourAPIì—ì„œ ì§ì ‘ ì œê³µë˜ì§€ ì•ŠìŒ
                        "category": "Culture", # Gemini ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
                    }
                    all_places.append(place)
            else:
                print(f"âŒ TourAPI ìš”ì²­ ì‹¤íŒ¨ (HTTP {response.status_code})")
        except Exception as e:
            print(f"âŒ TourAPI ì˜¤ë¥˜: {e}")

    # í†µí•© í›„ ì¡°íšŒìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•˜ê±°ë‚˜ ì„ì–´ì„œ ìƒìœ„ Nê°œ ì¶”ì¶œ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í•©ì¹¨)
    # ì‹¤ì œë¡œëŠ” êµ­ë¬¸ ì„œë¹„ìŠ¤ì—ì„œ ì¡°íšŒìˆ˜ë¥¼ ê°€ì ¸ì™€ì•¼ ì •í™•í•˜ì§€ë§Œ, 
    # EngServiceì˜ arrange=Bë„ ì–´ëŠ ì •ë„ ì‘ë™í•¨.
    return all_places[:max_items]

async def enrich_place_data(model, places: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì¥ì†Œ ì •ë³´ ê°•í™” (ì˜ë¬¸ ë²ˆì—­, AI Story, Photo Spot, Klook Keyword)
    """
    if not places:
        return []

    print("\nğŸ¤– Gemini AIë¡œ ì¥ì†Œ ì •ë³´ ê°•í™” ì¤‘...")
    
    enriched_places = []
    
    for i, place in enumerate(places, 1):
        prompt = f"""
Analyze this South Korean tourist spot: "{place['name_en']}" (Address: {place['address_ko']})
Generate the following information in JSON format:
1. "name_en": Official or well-known English name.
2. "ai_story": A fascinating 2-line story about its historical or cultural context.
3. "photo_spot": A specific tip for the best "Pro Photo Spot" angle (Instagrammable).
4. "tags": An array of 3 short keywords representing the vibe (e.g., ["Nature", "Hiking", "Autumn"]).
5. "category": One of these three categories: "Culture" (for palaces, temples, history), "Nature" (for mountains, beaches, parks), or "Modern" (for shopping, cafes, urban).
6. "hype_score": A score from 1-100 based on current SNS trend, Naver Map "Saves" level, and global search volume.
7. "klook_keyword": A search keyword for Klook (e.g., "Nami Island Tour").
8. "image_query": A high-quality English search keyword to find a representative photo of this place (e.g., "Gyeongbokgung Palace", "Myeongdong Night").

JSON only.
"""
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            if result_text.startswith('```'):
                result_text = result_text.split('```')[1]
                if result_text.startswith('json'):
                    result_text = result_text[4:]
            
            ai_data = json.loads(result_text)
            
            place["name_en"] = ai_data.get("name_en", place["name_en"])
            place["ai_story"] = ai_data.get("ai_story", "")
            place["photo_spot"] = ai_data.get("photo_spot", "")
            place["tags"] = ai_data.get("tags", ["Must Visit", "South Korea", "Trending"])
            place["category"] = ai_data.get("category", "Culture")
            ai_hype_score = ai_data.get("hype_score", 50)
            # 2. Hybrid Verification with Google Places Data
            google_data = get_google_place_stats(place["name_en"], place["address_ko"])
            
            if google_data:
                g_rating = google_data.get("rating", 0)
                g_reviews = google_data.get("userRatingCount", 0)
                # Google Hype Score = (Rating/5) * log10(Reviews+1) * 20 
                # (ìµœëŒ€ ì ìˆ˜ë¥¼ ì•½ 100ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ì„¤ê³„)
                google_hype = (g_rating / 5.0) * min(5, math.log10(g_reviews + 1)) * 20
                final_hype_score = int((google_hype * 0.7) + (ai_hype_score * 0.3))
                place["google_maps_url"] = google_data.get("googleMapsUri", "")
                print(f"    ğŸ” Google Verified: Rating {g_rating}, Reviews {g_reviews} -> Score: {int(google_hype)}")
            else:
                final_hype_score = ai_hype_score # Fallback to AI estimation
            
            # Commercial Match Bonus
            commercial_keywords = ["Palace", "Nami Island", "DMZ", "Seongsu", "Hannam", "Lotte World", "Everland"]
            commercial_bonus = 20 if any(kw.lower() in place["name_en"].lower() for kw in commercial_keywords) else 0
            
            # Final Score Calculation (Mixed Engine)
            # FinalScore = (TourAPI_Views_Index * 0.4) + (Hybrid_Hype_Score * 0.4) + (Commercial_Bonus * 0.2)
            try:
                base_views = int(place.get("views", 0)) if place.get("views") != "N/A" else 0
            except:
                base_views = 0
            
            # Normalize views index (relative)
            # 10,000 views -> approx 40 points in weighting
            views_weight = min(40, (base_views / 10000.0) * 40)
            
            place["final_score"] = views_weight + (final_hype_score * 0.4) + (commercial_bonus)
            place["hype_score"] = final_hype_score

            # Link & Data Generation
            klook_keyword = ai_data.get("klook_keyword", place["name_en"])
            place["klook_url"] = f"https://www.klook.com/en-US/search?query={klook_keyword.replace(' ', '%20')}&action=search"
            place["creatrip_url"] = f"https://www.creatrip.com/en/search?keyword={place['name_en'].replace(' ', '%20')}"
            
            # Determine priority_platform
            if place["category"] == "Culture":
                place["priority_platform"] = "Creatrip"
                place["booking_url"] = place["creatrip_url"]
            else:
                place["priority_platform"] = "Klook"
                place["booking_url"] = place["klook_url"]

            # Image logic enhancement: Use Google Photo > TourAPI > AI Fallback
            api_key = os.getenv('GOOGLE_PLACES_API_KEY') or os.getenv('GOOGLE_MAPS_API_KEY')
            
            # 1. Check if Google has a real photo
            if google_data and google_data.get("photo_name") and api_key:
                photo_ref = google_data["photo_name"]
                place["imageUrl"] = f"https://places.googleapis.com/v1/{photo_ref}/media?maxWidthPx=1200&maxHeightPx=800&key={api_key}"
                print(f"    ğŸ“¸ Image Source: Google Places (Verified)")
            # 2. If no Google photo, Check if TourAPI is valid
            elif not place.get("imageUrl") or "photo-1544273677-277914bd9466" in place["imageUrl"]:
                # 3. Last Resort: AI Guided category fallback
                print(f"    âš ï¸ Image Source: AI Category Fallback (TourAPI/Google missing)")
                category_id_map = {
                    "Culture": "1544273677-277914bd9466",
                    "Nature": "1538332152395-65715509746e",
                    "Modern": "1538485399081-7191377e8241"
                }
                cid = category_id_map.get(place["category"], "1544273677-277914bd9466")
                place["imageUrl"] = f"https://images.unsplash.com/photo-{cid}?w=800&fit=crop"
            else:
                print(f"    ğŸ–¼ï¸ Image Source: TourAPI (External)")

            place["verified_by_mix"] = True
            enriched_places.append(place)
            print(f"  âœ… {i}. {place['name_en']} ê°•í™” ì™„ë£Œ (Hype: {ai_hype_score}, Bonus: {commercial_bonus})")
        except Exception as e:
            print(f"âš ï¸ {place['name_en']} ê°•í™” ì¤‘ ì˜¤ë¥˜: {e}")
            place["ai_story"] = "Discover the hidden gem of Korea."
            place["photo_spot"] = "Best captured at golden hour."
            place["tags"] = ["Must Visit", "South Korea"]
            place["booking_url"] = f"https://www.klook.com/en-US/search?query={place['name_en'].replace(' ', '%20')}&action=search"
            place["hype_score"] = 50
            place["final_score"] = 50
            place["verified_by_mix"] = False
            enriched_places.append(place)
            enriched_places.append(place)

    # Sort by final_score descending
    enriched_places.sort(key=lambda x: x.get("final_score", 0), reverse=True)
    
    # Re-assign rank
    for i, place in enumerate(enriched_places, 1):
        place["rank"] = i
        
    return enriched_places

async def calculate_place_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Place ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_place"
        
        print(f"\nğŸ“Š Place íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday})")
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            for item in current_items:
                item['trend'] = 0
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        
        for current in current_items:
            # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item.get('name_en') == current.get('name_en'):
                    yesterday_rank = old_item.get('rank')
                    break
            
            if yesterday_rank:
                current['trend'] = yesterday_rank - current['rank']
            else:
                current['trend'] = 0
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ Place íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        for item in current_items:
            item['trend'] = 0
        return current_items

async def calculate_media_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë¯¸ë””ì–´ ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_media"
        
        print(f"\nğŸ“Š Media íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday})")
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ Media ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            for item in current_items:
                item['trend'] = 0
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ Media ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current in current_items:
            title_en = current.get('titleEn', '')
            title_ko = current.get('titleKo', '')
            current_rank = current['rank']
            
            # ì˜ì–´ ì œëª© ë˜ëŠ” í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if (old_item.get('titleEn') == title_en or 
                    old_item.get('titleKo') == title_ko):
                    yesterday_rank = old_item.get('rank')
                    break
            
            if yesterday_rank:
                trend = yesterday_rank - current_rank
                current['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {title_ko or title_en}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                current['trend'] = 0
                trend_changes.append(f"  {title_ko or title_en}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥
        if trend_changes:
            print("ğŸ“ˆ Media íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        for item in current_items:
            item['trend'] = 0
        return current_items


def save_to_firebase(db, category_key: str, products: List[Dict[str, Any]]):
    """
    Firebase Firestoreì— ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì €ì¥
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤ (ì˜ˆ: 'all', 'skincare', 'suncare')
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ’¾ Firebaseì— {category_key} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì¤‘...")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ (UTC)
    today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
    
    # Firestore ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
    firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
    
    # ë¬¸ì„œ ID: {ë‚ ì§œ}_{ì¹´í…Œê³ ë¦¬}
    doc_id = f"{today}_{firestore_category}"
    doc_ref = db.collection('daily_rankings').document(doc_id)
    
    # ë°ì´í„° êµ¬ì¡°
    data = {
        'date': today,
        'category': firestore_category,
        'items': products,
        'updatedAt': firestore.SERVER_TIMESTAMP
    }
    
    # ì €ì¥
    doc_ref.set(data)
    
    print(f"âœ… {len(products)}ê°œ ì œí’ˆì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
    print(f"ğŸ“ ì»¬ë ‰ì…˜: daily_rankings")
    print(f"ğŸ“„ ë¬¸ì„œ ID: {doc_id}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· K-Rank Scraper - Beauty & Media")
    print("=" * 60)
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì í™•ì¸
    run_mode = sys.argv[1] if len(sys.argv) > 1 else "all"  # "beauty", "media", "place", "all"
    
    try:
        # 1. Firebase ì´ˆê¸°í™”
        print("\nğŸ“± Firebase ì´ˆê¸°í™” ì¤‘...")
        db = initialize_firebase()
        print("âœ… Firebase ì—°ê²° ì™„ë£Œ")
        
        # 2. Gemini ì´ˆê¸°í™”
        print("\nğŸ¤– Gemini AI ì´ˆê¸°í™” ì¤‘...")
        model = initialize_gemini()
        print("âœ… Gemini AI ì—°ê²° ì™„ë£Œ")
        
        total_products = 0
        
        # 3. Beauty ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["beauty", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ’„ BEAUTY ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§")
            print("=" * 60)
            
            for category_key, config in CATEGORY_MAPPING.items():
                print("\n" + "-" * 60)
                print(f"ğŸ“¦ {category_key.upper()} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
                print("-" * 60)
                
                # ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ (ë¬´ì‹ ì‚¬ ë·°í‹° ìƒìœ„ 20ê°œ)
                products = scrape_musinsa_beauty_by_category(
                    category_code=config['url_param'],
                    max_items=20
                )
                
                if not products:
                    print(f"âš ï¸  {category_key} ì¹´í…Œê³ ë¦¬ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # ë¸Œëœë“œëª… ì˜ì–´ ë³€í™˜ (ë¨¼ì € ì‹¤í–‰)
                products = translate_brand_names(products)
                
                # ì œí’ˆëª… ì˜ì–´ ë²ˆì—­ (Batch Processing)
                products = await translate_product_names_batch(model, products)
                
                # ì•„ë§ˆì¡´ ì´ë¯¸ì§€ ì—°ë™
                print("\nğŸ“¸ ì•„ë§ˆì¡´ì—ì„œ ì œí’ˆ ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...")
                for product in products:
                    search_query = f"{product['brand']} {product['productName']}"
                    amazon_img = await get_amazon_image(search_query)
                    if amazon_img:
                        product['imageUrl'] = amazon_img
                        print(f"  âœ… {product['rank']}ìœ„ ì´ë¯¸ì§€ ë§¤ì¹­ ì„±ê³µ")
                    else:
                        print(f"  âš ï¸ {product['rank']}ìœ„ ì•„ë§ˆì¡´ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
                
                # íŠ¸ë Œë“œ ê³„ì‚° (ë²ˆì—­ í›„ ì‹¤í–‰í•˜ì—¬ ì˜ì–´ ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­)
                products = await calculate_trends(db, category_key, products)
                
                # íƒœê·¸ ìë™ ìƒì„±
                products = await generate_tags(model, products, category_key)
                
                # Firebaseì— ì €ì¥
                save_to_firebase(db, category_key, products)
                total_products += len(products)
        
        # 4. Media ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["media", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ¬ MEDIA ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (Netflix)")
            print("=" * 60)
            
            all_media_items = []
            
            # Netflix TV Shows Top 10 í¬ë¡¤ë§
            print("\nğŸ“º Netflix TV Shows í¬ë¡¤ë§ ì¤‘...")
            tv_items = await scrape_netflix(media_type='tv', max_items=10)
            if tv_items:
                all_media_items.extend(tv_items)
                print(f"âœ… TV Shows {len(tv_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ TV Shows ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # Netflix Films Top 10 í¬ë¡¤ë§
            print("\nğŸ¬ Netflix Films í¬ë¡¤ë§ ì¤‘...")
            film_items = await scrape_netflix(media_type='films', max_items=10)
            if film_items:
                all_media_items.extend(film_items)
                print(f"âœ… Films {len(film_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ Films ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            if all_media_items:
                # í•œêµ­ì–´ ì œëª© ë²ˆì—­ (ë¨¼ì € ì‹¤í–‰)
                all_media_items = await translate_media_titles(model, all_media_items)
                
                # íŠ¸ë Œë“œ ê³„ì‚° (ë²ˆì—­ í›„ ì‹¤í–‰í•˜ì—¬ ì˜ì–´/í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë§¤ì¹­)
                all_media_items = await calculate_media_trends(db, all_media_items)
                
                # Media ì €ì¥ ë¡œì§
                today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
                doc_id = f"{today}_media"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'media',
                    'items': all_media_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                doc_ref.set(data)
                print(f"âœ… {len(all_media_items)}ê°œ íƒ€ì´í‹€ì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                print(f"   - TV Shows: {len(tv_items)}ê°œ")
                print(f"   - Films: {len(film_items)}ê°œ")
                total_products += len(all_media_items)
            else:
                print("âš ï¸ Netflixì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 5. Place ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["place", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ—ºï¸  PLACE ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (TourAPI)")
            print("=" * 60)
            
            # TourAPI ìˆ˜ì§‘
            place_items = await scrape_tour_api(max_items=50)
            
            if place_items:
                # Gemini AI ê°•í™”
                place_items = await enrich_place_data(model, place_items)
                
                # íŠ¸ë Œë“œ ê³„ì‚°
                place_items = await calculate_place_trends(db, place_items)
                
                # ì €ì¥
                today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
                doc_id = f"{today}_place"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'place',
                    'items': place_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                doc_ref.set(data)
                print(f"âœ… {len(place_items)}ê°œ ëª…ì†Œë¥¼ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                total_products += len(place_items)
            else:
                print("âš ï¸ TourAPIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"  - ì´ ì•„ì´í…œ ìˆ˜: {total_products}ê°œ")
        print(f"  - ì‹¤í–‰ ëª¨ë“œ: {run_mode.upper()}")

        # ë°ì´í„° ê²€ì¦: ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        if total_products == 0:
            print(f"\nâŒ [CRITICAL] {run_mode.upper()} ëª¨ë“œì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
            print("ğŸ’¡ í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì ‘ê·¼ì´ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # ì‚¬ìš©ë²•:
    # python scraper.py           # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‹¤í–‰
    # python scraper.py beauty    # Beautyë§Œ ì‹¤í–‰
    # python scraper.py media     # Mediaë§Œ ì‹¤í–‰
    asyncio.run(main())
