#!/usr/bin/env python3
"""
K-Rank Beauty Scraper
ì˜¬ë¦¬ë¸Œì˜ ë² ìŠ¤íŠ¸ ì œí’ˆ ë­í‚¹ì„ í¬ë¡¤ë§í•˜ê³  Firebaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import random
import time
from datetime import datetime
from typing import List, Dict, Any
import json

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ì˜
CATEGORY_MAPPING = {
    'all': {'url_param': None, 'firestore_category': 'beauty'},
    'skincare': {'url_param': '10000010001', 'firestore_category': 'beauty-skincare'},
    'suncare': {'url_param': '10000010011', 'firestore_category': 'beauty-suncare'},
    'masks': {'url_param': '10000010009', 'firestore_category': 'beauty-masks'},
    'makeup': {'url_param': '10000010002', 'firestore_category': 'beauty-makeup'},
    'haircare': {'url_param': '10000010004', 'firestore_category': 'beauty-haircare'},
    'bodycare': {'url_param': '10000010003', 'firestore_category': 'beauty-bodycare'},
}


# Firebase ì´ˆê¸°í™”
def initialize_firebase():
    """Firebase Admin SDK ì´ˆê¸°í™”"""
    if not firebase_admin._apps:
        cred = credentials.Certificate('serviceAccountKey.json')
        firebase_admin.initialize_app(cred)
    return firestore.client()

# Gemini API ì´ˆê¸°í™”
def initialize_gemini():
    """Gemini API ì´ˆê¸°í™”"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in .env file")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('models/gemini-2.5-flash')

def scrape_olive_young_by_category(category_code: str = None, max_items: int = 20, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    ì˜¬ë¦¬ë¸Œì˜ ì¹´í…Œê³ ë¦¬ë³„ ë² ìŠ¤íŠ¸ ì œí’ˆ í¬ë¡¤ë§ (ScraperAPI ì‚¬ìš©)
    
    Args:
        category_code: ì¹´í…Œê³ ë¦¬ ì½”ë“œ (ì˜ˆ: '10000010001' for Skincare, None for All)
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜
        max_retries: ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    # ScraperAPI í‚¤ í™•ì¸
    scraperapi_key = os.getenv('SCRAPERAPI_KEY')
    if not scraperapi_key:
        print("âŒ SCRAPERAPI_KEY not found in environment")
        return products
    
    # URL ìƒì„±
    if category_code:
        target_url = f"https://www.oliveyoung.co.kr/store/main/getBestList.do?dispCatNo=900000100100001&fltDispCatNo={category_code}&rowsPerPage=100"
    else:
        target_url = "https://www.oliveyoung.co.kr/store/main/getBestList.do?dispCatNo=900000100100001&rowsPerPage=100"
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸŒ ScraperAPIë¡œ í˜ì´ì§€ ìš”ì²­ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
            print(f"ğŸ“„ URL: {target_url}")
            
            # ScraperAPI íŒŒë¼ë¯¸í„°
            params = {
                'api_key': scraperapi_key,
                'url': target_url,
                'country_code': 'kr',  # í•œêµ­ IP ì‚¬ìš©
                'render': 'true'  # JavaScript ë Œë”ë§
            }
            
            # ìš”ì²­ ì „ì†¡
            response = requests.get('http://api.scraperapi.com', params=params, timeout=60)
            
            if response.status_code == 200:
                print("âœ… ScraperAPI ìš”ì²­ ì„±ê³µ!")
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Cloudflare ì²´í¬
                page_title = soup.title.string if soup.title else "No Title"
                if "ì ì‹œë§Œ" in page_title or "Just a moment" in page_title:
                    print(f"âš ï¸  ì—¬ì „íˆ Cloudflare í˜ì´ì§€ ê°ì§€ë¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        time.sleep(5)
                        continue
                    else:
                        print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return products
                
                print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {page_title}")
                
                # ì œí’ˆ íŒŒì‹±
                items = soup.select('div.prd_info')[:max_items]
                
                if len(items) == 0:
                    print("âš ï¸  'div.prd_info'ë¡œ ì œí’ˆì„ ì°¾ì§€ ëª»í•¨")
                    items = soup.select('ul.common_prd_list li')[:max_items]
                
                print(f"âœ… {len(items)}ê°œ ì œí’ˆ ë°œê²¬")
                
                if len(items) == 0:
                    print(f"âš ï¸  ì œí’ˆì„ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        time.sleep(5)
                        continue
                    else:
                        print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return products
                
                # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
                for idx, item in enumerate(items, 1):
                    try:
                        name_elem = item.select_one('.prd_name .tx_name') or item.select_one('p.tx_name')
                        name = name_elem.get_text(strip=True) if name_elem else f"Product {idx}"
                        
                        brand_elem = item.select_one('.tx_brand')
                        brand = brand_elem.get_text(strip=True) if brand_elem else "Unknown"
                        
                        img_elem = item.select_one('img')
                        image_url = ''
                        if img_elem:
                            image_url = img_elem.get('src', '') or img_elem.get('data-original', '')
                        if image_url and not image_url.startswith('http'):
                            image_url = 'https:' + image_url
                        
                        price_elem = item.select_one('.tx_cur .tx_num')
                        price = price_elem.get_text(strip=True) if price_elem else "0"
                        if price:
                            price = price + "ì›"
                        
                        link_elem = item.select_one('a')
                        buy_url = link_elem.get('href', '') if link_elem else ''
                        if buy_url and not buy_url.startswith('http'):
                            buy_url = 'https://www.oliveyoung.co.kr' + buy_url
                        
                        product = {
                            'rank': idx,
                            'productName': name,
                            'brand': brand,
                            'imageUrl': image_url or "https://images.unsplash.com/photo-1620916566398-39f1143ab7be?w=100&h=100&fit=crop",
                            'price': price,
                            'buyUrl': buy_url,
                            'tags': [],
                            'subcategory': 'skincare',
                            'trend': 0
                        }
                        
                        products.append(product)
                        print(f"  {idx}. {brand} - {name} ({price})")
                        
                    except Exception as e:
                        print(f"âš ï¸  ì œí’ˆ {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                print("âœ… ì œí’ˆ í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
            else:
                print(f"âŒ ScraperAPI ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                    time.sleep(5)
                    continue
                else:
                    print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    
        except requests.exceptions.Timeout:
            print(f"â±ï¸  ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                time.sleep(5)
                continue
            else:
                print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                time.sleep(5)
                continue
            else:
                print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                import traceback
                traceback.print_exc()
    
    return products
async def calculate_trends(db, category_key: str, current_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì´ì „ ë‚ ì§œ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ íŠ¸ë Œë“œ ê³„ì‚°
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤
        current_products: í˜„ì¬ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        íŠ¸ë Œë“œê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    from datetime import timedelta
    
    try:
        # ì–´ì œ ë‚ ì§œ (UTC)
        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
        doc_id = f"{yesterday}_{firestore_category}"
        
        # ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            # ì–´ì œ ë°ì´í„° ì—†ìœ¼ë©´ íŠ¸ë Œë“œ 0
            for product in current_products:
                product['trend'] = 0
            return current_products
        
        yesterday_items = doc.to_dict().get('items', [])
        
        # ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ìˆœìœ„ ë³€ë™ ê³„ì‚°
        for current_item in current_products:
            current_rank = current_item['rank']
            product_name = current_item['productName']
            
            # ì–´ì œ ìˆœìœ„ ì°¾ê¸°
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item['productName'] == product_name:
                    yesterday_rank = old_item['rank']
                    break
            
            if yesterday_rank:
                # íŠ¸ë Œë“œ = ì–´ì œ ìˆœìœ„ - ì˜¤ëŠ˜ ìˆœìœ„ (ì–‘ìˆ˜ë©´ ìƒìŠ¹)
                current_item['trend'] = yesterday_rank - current_rank
            else:
                # ì‹ ê·œ ì§„ì…
                current_item['trend'] = 0
        
        return current_products
        
    except Exception as e:
        print(f"âš ï¸  íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •
        for product in current_products:
            product['trend'] = 0
        return current_products

async def translate_to_english(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆëª…ê³¼ ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë²ˆì—­
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì˜ì–´ë¡œ ë²ˆì—­ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ Gemini AIë¡œ ì œí’ˆëª… ë° ë¸Œëœë“œëª… ì˜ì–´ ë²ˆì—­ ì¤‘...")
    
    # ì œí’ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    product_names = [f"{p['rank']}. {p['brand']} - {p['productName']}" for p in products]
    
    prompt = f"""
Translate the following Korean beauty product brands and names to English.
Romanize Korean brand names (e.g., ë©”ë””í â†’ Mediheal, ì–´ë…¸ë¸Œ â†’ UNOVE).
Remove special characters like [], ê¸°íš, ë‹¨í’ˆ, etc.
Make the names concise and clear.

Products:
{chr(10).join(product_names)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "brand": "English Brand Name", "product_name": "English Product Name"}},
    {{"rank": 2, "brand": "English Brand Name", "product_name": "English Product Name"}},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ì œí’ˆì— ì˜ì–´ ì´ë¦„ ë° ë¸Œëœë“œ ì ìš©
        for item in translations.get('translations', []):
            rank = item.get('rank')
            english_brand = item.get('brand', '')
            english_name = item.get('product_name', '')
            
            for product in products:
                if product['rank'] == rank:
                    # í•œê¸€ ë¸Œëœë“œì™€ ì œí’ˆëª…ì„ ì˜ì–´ë¡œ ì™„ì „íˆ êµì²´
                    if english_brand:
                        product['brand'] = english_brand
                    if english_name:
                        product['productName'] = english_name
                    break
        
        print("âœ… ì˜ì–´ ë²ˆì—­ ì™„ë£Œ (ë¸Œëœë“œ + ì œí’ˆëª…)")
        
    except Exception as e:
        print(f"âš ï¸  Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
        print("í•œê¸€ ì œí’ˆëª… ìœ ì§€")
    
    return products

async def generate_tags(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆë³„ íƒœê·¸ ìë™ ìƒì„±
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        íƒœê·¸ê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸ·ï¸  Gemini AIë¡œ ì œí’ˆ íƒœê·¸ ìë™ ìƒì„± ì¤‘...")
    
    # ì œí’ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì˜ì–´ ë²ˆì—­ëœ ì´ë¦„ ì‚¬ìš©)
    product_info = [f"{p['rank']}. {p['brand']} - {p['productName']}" for p in products]
    
    prompt = f"""
Generate 2-3 relevant tags for each beauty product.
Tags should describe product benefits, type, or main features.
Use English tags only. Keep them short and concise.

Examples:
- Mask Pack â†’ ["Hydrating", "Soothing", "Sheet Mask"]
- Hair Treatment â†’ ["Damage Repair", "Moisturizing"]
- Sunscreen â†’ ["UV Protection", "Tone Up"]

Products:
{chr(10).join(product_info)}

Response format (JSON):
{{
  "tags": [
    {{"rank": 1, "tags": ["Hydrating", "Soothing", "Sheet Mask"]}},
    {{"rank": 2, "tags": ["Damage Repair", "Moisturizing"]}},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        tag_data = json.loads(result_text)
        
        # ì œí’ˆì— íƒœê·¸ ì ìš©
        for item in tag_data.get('tags', []):
            rank = item.get('rank')
            tags = item.get('tags', [])
            
            for product in products:
                if product['rank'] == rank:
                    product['tags'] = tags
                    break
        
        print("âœ… íƒœê·¸ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸  Gemini íƒœê·¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print("ë¹ˆ íƒœê·¸ ë°°ì—´ ìœ ì§€")
    
    return products

async def scrape_netflix(media_type: str = 'tv', max_items: int = 10, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    Netflix Top 10 South Korea TV Shows/Films í¬ë¡¤ë§
    
    Args:
        media_type: 'tv' ë˜ëŠ” 'film'
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸ 10ê°œ)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    for attempt in range(max_retries):
        try:
            async with async_playwright() as p:
                print(f"ğŸ¬ Netflix Top 10 í¬ë¡¤ë§ ì‹œì‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )
                
                page = await context.new_page()
                
                # Netflix Top 10 URL (tv ë˜ëŠ” films)
                url = f"https://top10.netflix.com/south-korea/{media_type}"
                print(f"ğŸ“„ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                
                await page.goto(url, wait_until='networkidle', timeout=60000)
                
                # í…Œì´ë¸”ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    await page.wait_for_selector("table tbody tr", timeout=30000)
                except:
                    print("âš ï¸ í…Œì´ë¸” ì…€ë ‰í„° ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
                
                await page.wait_for_timeout(3000)  # ì¶”ê°€ ë Œë”ë§ ëŒ€ê¸°
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # í…Œì´ë¸” í–‰(Row) ì„ íƒ
                rows = soup.select("table tbody tr")[:max_items]
                print(f"âœ… {len(rows)}ê°œ íƒ€ì´í‹€ ë°œê²¬!")
                
                if len(rows) == 0:
                    print(f"âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    await browser.close()
                    if attempt < max_retries - 1:
                        await asyncio.sleep(5)
                        continue
                    else:
                        return products
                
                for i, row in enumerate(rows, 1):
                    try:
                        # ë¸Œë¼ìš°ì € ë¶„ì„ ê¸°ë°˜ ì…€ë ‰í„°
                        rank_el = row.select_one("span.rank")
                        title_el = row.select_one("td.title button")
                        weeks_el = row.select_one("td[data-uia='top10-table-row-weeks']")
                        img_el = row.select_one("td.title img.desktop-only")
                        
                        rank_text = rank_el.get_text(strip=True) if rank_el else str(i)
                        title = title_el.get_text(strip=True) if title_el else f"Unknown Title {i}"
                        weeks = weeks_el.get_text(strip=True) if weeks_el else "1"
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = img_el.get('src', '') if img_el else 'https://assets.nflxext.com/us/ffe/siteui/common/icons/nficon2016.png'
                        
                        # YouTube íŠ¸ë ˆì¼ëŸ¬ ë§í¬ ìƒì„±
                        trailer_query = f"{title} trailer"
                        trailer_link = f"https://www.youtube.com/results?search_query={trailer_query.replace(' ', '+')}"
                        
                        # media_typeì— ë”°ë¼ type ì„¤ì •
                        item_type = 'TV Show' if media_type == 'tv' else 'Film'
                        default_tag = 'K-Drama' if media_type == 'tv' else 'Korean Film'
                        
                        item = {
                            'rank': int(rank_text) if rank_text.isdigit() else i,
                            'titleEn': title,
                            'titleKo': title,  # ì´í›„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸
                            'imageUrl': image_url,
                            'weeksInTop10': weeks,
                            'type': item_type,
                            'trailerLink': trailer_link,
                            'vpnLink': 'https://nordvpn.com/ko/',
                            'tags': [f"{weeks} Weeks in Top 10", default_tag],
                            'trend': 0
                        }
                        
                        products.append(item)
                        print(f"  {rank_text}ìœ„. {title} ({weeks}ì£¼ ì—°ì† Top 10)")
                        
                    except Exception as e:
                        print(f"âš ï¸ {i}ìœ„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                await browser.close()
                print("âœ… Netflix í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            else:
                import traceback
                traceback.print_exc()
    
    return products

async def translate_media_titles(model, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª©(Netflix)ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    """
    print("\nğŸŒ Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª© í•œêµ­ì–´ ë²ˆì—­ ì¤‘...")
    
    # ì œëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    titles = [f"{item['rank']}. {item['titleEn']}" for item in items]
    
    prompt = f"""
Translate the following Netflix TV Show/Film titles into their official Korean titles.
Some are already Korean dramas, so find their original Korean titles (e.g., 'Squid Game' -> 'ì˜¤ì§•ì–´ ê²Œì„').
Exclude rank numbers from the translation.

Titles:
{chr(10).join(titles)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    {{"rank": 2, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    ...
  ]
}}

JSON only.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ì ìš©
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            title_ko = trans.get('titleKo')
            
            for item in items:
                if item['rank'] == rank:
                    item['titleKo'] = title_ko
                    break
        
        print("âœ… ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
    
    return items

async def calculate_media_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë¯¸ë””ì–´ ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_media"
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        
        for current in current_items:
            title = current['titleEn']
            yesterday_rank = next((item['rank'] for item in yesterday_items if item['titleEn'] == title), None)
            
            if yesterday_rank:
                current['trend'] = yesterday_rank - current['rank']
            else:
                current['trend'] = 0
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return current_items


def save_to_firebase(db, category_key: str, products: List[Dict[str, Any]]):
    """
    Firebase Firestoreì— ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì €ì¥
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤ (ì˜ˆ: 'all', 'skincare', 'suncare')
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ’¾ Firebaseì— {category_key} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì¤‘...")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ (UTC)
    today = datetime.utcnow().strftime('%Y-%m-%d')
    
    # Firestore ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
    firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
    
    # ë¬¸ì„œ ID: {ë‚ ì§œ}_{ì¹´í…Œê³ ë¦¬}
    doc_id = f"{today}_{firestore_category}"
    doc_ref = db.collection('daily_rankings').document(doc_id)
    
    # ë°ì´í„° êµ¬ì¡°
    data = {
        'date': today,
        'category': firestore_category,
        'items': products,
        'updatedAt': firestore.SERVER_TIMESTAMP
    }
    
    # ì €ì¥
    doc_ref.set(data)
    
    print(f"âœ… {len(products)}ê°œ ì œí’ˆì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
    print(f"ğŸ“ ì»¬ë ‰ì…˜: daily_rankings")
    print(f"ğŸ“„ ë¬¸ì„œ ID: {doc_id}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· K-Rank Scraper - Beauty & Media")
    print("=" * 60)
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì í™•ì¸
    run_mode = sys.argv[1] if len(sys.argv) > 1 else "all"  # "beauty", "media", "all"
    
    try:
        # 1. Firebase ì´ˆê¸°í™”
        print("\nğŸ“± Firebase ì´ˆê¸°í™” ì¤‘...")
        db = initialize_firebase()
        print("âœ… Firebase ì—°ê²° ì™„ë£Œ")
        
        # 2. Gemini ì´ˆê¸°í™”
        print("\nğŸ¤– Gemini AI ì´ˆê¸°í™” ì¤‘...")
        model = initialize_gemini()
        print("âœ… Gemini AI ì—°ê²° ì™„ë£Œ")
        
        total_products = 0
        
        # 3. Beauty ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["beauty", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ’„ BEAUTY ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§")
            print("=" * 60)
            
            for category_key, config in CATEGORY_MAPPING.items():
                print("\n" + "-" * 60)
                print(f"ğŸ“¦ {category_key.upper()} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
                print("-" * 60)
                
                # ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
                products = scrape_olive_young_by_category(
                    category_code=config['url_param'],
                    max_items=20
                )
                
                if not products:
                    print(f"âš ï¸  {category_key} ì¹´í…Œê³ ë¦¬ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # íŠ¸ë Œë“œ ê³„ì‚° (ì´ì „ ë‚ ì§œ ë°ì´í„°ì™€ ë¹„êµ)
                products = await calculate_trends(db, category_key, products)
                
                # ì˜ì–´ ë²ˆì—­ (ë¸Œëœë“œ + ì œí’ˆëª…)
                products = await translate_to_english(model, products)
                
                # íƒœê·¸ ìë™ ìƒì„±
                products = await generate_tags(model, products)
                
                # Firebaseì— ì €ì¥
                save_to_firebase(db, category_key, products)
                total_products += len(products)
        
        # 4. Media ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["media", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ¬ MEDIA ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (Netflix)")
            print("=" * 60)
            
            all_media_items = []
            
            # Netflix TV Shows Top 10 í¬ë¡¤ë§
            print("\nğŸ“º Netflix TV Shows í¬ë¡¤ë§ ì¤‘...")
            tv_items = await scrape_netflix(media_type='tv', max_items=10)
            if tv_items:
                all_media_items.extend(tv_items)
                print(f"âœ… TV Shows {len(tv_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ TV Shows ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # Netflix Films Top 10 í¬ë¡¤ë§
            print("\nğŸ¬ Netflix Films í¬ë¡¤ë§ ì¤‘...")
            film_items = await scrape_netflix(media_type='films', max_items=10)
            if film_items:
                all_media_items.extend(film_items)
                print(f"âœ… Films {len(film_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ Films ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            if all_media_items:
                # íŠ¸ë Œë“œ ê³„ì‚°
                all_media_items = await calculate_media_trends(db, all_media_items)
                
                # í•œêµ­ì–´ ì œëª© ë²ˆì—­
                all_media_items = await translate_media_titles(model, all_media_items)
                
                # Media ì €ì¥ ë¡œì§
                today = datetime.utcnow().strftime('%Y-%m-%d')
                doc_id = f"{today}_media"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'media',
                    'items': all_media_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                doc_ref.set(data)
                print(f"âœ… {len(all_media_items)}ê°œ íƒ€ì´í‹€ì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                print(f"   - TV Shows: {len(tv_items)}ê°œ")
                print(f"   - Films: {len(film_items)}ê°œ")
                total_products += len(all_media_items)
            else:
                print("âš ï¸ Netflixì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"  - ì´ ì•„ì´í…œ ìˆ˜: {total_products}ê°œ")
        print(f"  - ì‹¤í–‰ ëª¨ë“œ: {run_mode.upper()}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # ì‚¬ìš©ë²•:
    # python scraper.py           # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‹¤í–‰
    # python scraper.py beauty    # Beautyë§Œ ì‹¤í–‰
    # python scraper.py media     # Mediaë§Œ ì‹¤í–‰
    asyncio.run(main())
