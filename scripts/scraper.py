#!/usr/bin/env python3
"""
K-Rank Beauty Scraper
ì˜¬ë¦¬ë¸Œì˜ ë² ìŠ¤íŠ¸ ì œí’ˆ ë­í‚¹ì„ í¬ë¡¤ë§í•˜ê³  Firebaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import random
import time
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
import json
import math

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
import firebase_admin
from firebase_admin import credentials, firestore
import google.generativeai as genai
from dotenv import load_dotenv
from hangul_romanize import Transliter
from hangul_romanize.rule import academic

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
env_path = os.path.join(project_root, '.env')
load_dotenv(env_path)

# ê°œë°œ ëª¨ë“œ ë° ì œí•œ ì„¤ì •
DEV_MODE = os.getenv('DEV_MODE', 'false').lower() == 'true'
DEV_LIMIT = 5  # ê°œë°œ ëª¨ë“œì¼ ë•Œ ì²˜ë¦¬í•  ì•„ì´í…œ ìˆ˜
CACHE_FILE = os.path.join(script_dir, 'product_cache.json')
WRITE_TO_FIRESTORE = os.getenv('WRITE_TO_FIRESTORE', 'true').lower() == 'true'

# ìºì‹œ ë¡œë“œ/ì €ì¥ í•¨ìˆ˜
def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return {}

def save_cache(cache_data):
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ì˜ (í™”í•´ ê¸€ë¡œë²Œ ê¸°ì¤€)
# Theme IDs: All=2, Skincare=4, Suncare=34, Masks=25, Makeup=41, Haircare=82, Bodycare=68
CATEGORY_MAPPING = {
    'all': {'url_param': '2', 'firestore_category': 'beauty'},
    'skincare': {'url_param': '4', 'firestore_category': 'beauty-skincare'},
    'suncare': {'url_param': '34', 'firestore_category': 'beauty-suncare'},
    'masks': {'url_param': '25', 'firestore_category': 'beauty-masks'},
    'makeup': {'url_param': '41', 'firestore_category': 'beauty-makeup'},
    'haircare': {'url_param': '82', 'firestore_category': 'beauty-haircare'},
    'bodycare': {'url_param': '68', 'firestore_category': 'beauty-bodycare'},
}

# ë¸Œëœë“œëª… ì˜ì–´ ë§¤í•‘
BRAND_NAME_MAPPING = {
    # ì£¼ìš” ë¸Œëœë“œ
    'ë©”ë””íë¸Œ': 'Medicube',
    'ì—ìŠ¤ë„¤ì´ì²˜': 'S.Nature',
    'ì—ìŠ¤íŠ¸ë¼': 'AESTURA',
    'ì´ì¦ˆì•¤íŠ¸ë¦¬': 'Isntree',
    'ì›°ë¼ì¥¬': 'Wellage',
    'ë‹¬ë°”': "d'Alba",
    'ë©”ë””í': 'Mediheal',
    'ì„¤í™”ìˆ˜': 'Sulwhasoo',
    'ë¼ë¡œìŠˆí¬ì œ': 'La Roche-Posay',
    'í† ë¦¬ë“ ': 'Torriden',
    'ì•„ëˆ„ì•„': 'Anua',
    'ì°¨ì•¤ë°•': 'CNP Laboratory',
    'ë¸”ë‘ë„¤ì´ì²˜': 'BLANC NATURE',
    'í”„ë¦¬ë©”ë¼': 'Primera',
    'í•œìœ¨': 'Hanyul',
    'ì—ì´í”„ë¦´ìŠ¤í‚¨': 'April Skin',
    'ë§ˆë…€ê³µì¥': "Ma:nyo Factory",
    'í—¤ë¼': 'HERA',
    'ENHYPEN': 'ENHYPEN',
    'ìŠ¤í‚¨í‘¸ë“œ': 'SKINFOOD',
    'ë©”ë…¸í‚¨': 'Menokin',
    'ì˜ë‚´ì¶”ëŸ´': 'So Natural',
    'êµ¬ë‹¬': 'goodal',
    'ë‹¥í„°ì§€': 'Dr.G',
    'ì •ìƒ˜ë¬¼': 'JUNG SAEM MOOL',
    'í´ë¦¬ì˜¤': 'CLIO',
    'ë¡¬ì•¤': 'rom&nd',
    'í˜ë¦¬í˜ë¼': 'peripera',
    'ì–´ë…¸ë¸Œ': 'UNOVE',
    'ë‹¥í„°ê·¸ë£¨íŠ¸': 'Dr.Groot',
    'ë¯¸ìŸì„¼': 'MISE EN SCENE',
    'ì¼ë¦¬ìœ¤': 'illiyoon',
    'ì„¸íƒ€í•„': 'Cetaphil',
    'ë¼ìš´ë“œë©': 'Round Lab',
    'ë‹¥í„°í¬í—¤ì–´': 'Dr.FORHAIR',
    'ë¹„í”Œë ˆì¸': 'beplain',
    'ì½”ìŠ¤ì•Œì—‘ìŠ¤': 'COSRX',
    'ì¡°ì„ ë¯¸ë…€': 'Beauty of Joseon',
    'ì˜¤ë“œíƒ€ì…': 'ODE TYPE',
    'ë¸Œë§ê·¸ë¦°': 'BRING GREEN',
    'ë°”ì´ì˜¤ë”ë§ˆ': 'BIODERMA',
    'ìœ ë¦¬ì•„ì¥¬': 'URIAGE',
}



# Firebase ì´ˆê¸°í™”
def initialize_firebase():
    """Firebase Admin SDK ì´ˆê¸°í™”"""
    if not firebase_admin._apps:
        # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì™€ ìƒê´€ì—†ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ serviceAccountKey.json ì‚¬ìš©
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        key_path = os.path.join(project_root, 'serviceAccountKey.json')
        
        cred = credentials.Certificate(key_path)
        firebase_admin.initialize_app(cred)
    return firestore.client()


# Gemini API ì´ˆê¸°í™”
def initialize_gemini():
    """Gemini API ì´ˆê¸°í™”"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in .env file")
    genai.configure(api_key=api_key)
    # models/gemini-2.0-flash: ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ì´ë©° í• ë‹¹ëŸ‰ì´ ì•ˆì •ì ì„
    return genai.GenerativeModel('models/gemini-2.0-flash')


async def get_amazon_image(query: str) -> str:
    """
    ì•„ë§ˆì¡´ ê²€ìƒ‰ì„ í†µí•´ ì œí’ˆ ì´ë¯¸ì§€ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ê°•í™”ëœ ë²„ì „)
    """
    api_key = os.getenv('WEBSCRAPING_AI_API_KEY')
    if not api_key:
        return ""
    
    search_query = query.replace(' ', '+')
    search_url = f"https://www.amazon.com/s?k={search_query}"
    
    try:
        params = {
            'api_key': api_key,
            'url': search_url,
            'proxy': 'residential',
            'country': 'us'
        }
        
        print(f"ğŸ” Amazon ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘: {query}")
        response = requests.get('https://api.webscraping.ai/html', params=params, timeout=60)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³´ë‹¤ ìœ ì—°í•œ ì…€ë ‰í„° ì‹œë„
            selectors = [
                'div[data-component-type="s-search-result"] img.s-image',
                'img.s-image',
                'img[src*="media-amazon.com/images/I/"]'
            ]
            
            for selector in selectors:
                img_elems = soup.select(selector)
                for img in img_elems:
                    src = img.get('src', '')
                    if src and ('images-na.ssl-images-amazon.com' in src or 'm.media-amazon.com' in src) and 'gif' not in src:
                        # ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í¬ê¸° ì˜µì…˜ ì œê±°)
                        # ì˜ˆ: ...._AC_UL320_.jpg -> ....jpg
                        high_res_src = re.sub(r'\._AC_.*?_\.', '.', src)
                        return high_res_src
                    
    except Exception as e:
        print(f"âš ï¸ Amazon ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
    
    return ""




async def scrape_hwahae_global(url: str, max_items: int = 20) -> List[Dict[str, Any]]:
    """
    í™”í•´ ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ì œí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ì˜ë¬¸ ì‚¬ì´íŠ¸ì—ì„œ í•œê¸€ ë¦¬ë·°ë¥¼ í¬í•¨í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    products = []
    try:
        async with async_playwright() as p:
            print(f"ğŸŒ í™”í•´ ê¸€ë¡œë²Œ ì ‘ì† ì¤‘: {url}")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì¼ì‹œì ìœ¼ë¡œ headless=False ì‹œë„ ê°€ëŠ¥ (í•„ìš”ì‹œ)
            browser = await p.chromium.launch(headless=True)
            # ì‹¤ì œ ì‚¬ìš©ìì˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê¸° ìœ„í•´ User-Agent ê°•í™”
            user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36'
            context = await browser.new_context(
                user_agent=user_agent,
                locale='en-US'
            )
            page = await context.new_page()
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            print("â³ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ, ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(5) # ë„‰ë„‰í•˜ê²Œ ëŒ€ê¸°
            
            # ì§€ì—° ë¡œë”© ë° 20ê°œ ì´ìƒ ë¡œë“œë˜ë„ë¡ ìŠ¤í¬ë¡¤
            print("ğŸ“œ ìŠ¤í¬ë¡¤ ì¤‘...")
            for _ in range(2):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            
            # ì œí’ˆ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ê°œë°œ ëª¨ë“œì¼ ë•ŒëŠ” ìˆ˜ì§‘ ìˆ˜ëŸ‰ ì œí•œ
            current_max = DEV_LIMIT if DEV_MODE else max_items
            items = soup.select('li.mt-16.bg-white')[:current_max]
            
            print(f"ğŸ” ë°œê²¬ëœ ì œí’ˆ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(items)} (DEV_MODE: {DEV_MODE}, Limit: {current_max})")
            
            for idx, item in enumerate(items, 1):
                try:
                    # ë©”ì¸ ë§í¬ ë° ë°ì´í„° ì˜ì—­
                    link_elem = item.select_one('a.flex.items-center[href*="/en/products/"]')
                    if not link_elem:
                        continue
                        
                    # 1. ìˆœìœ„ ì¶”ì¶œ (1-3ìœ„ ë©”ë‹¬ ì•„ì´ì½˜ vs 4ìœ„ ì´í•˜ í…ìŠ¤íŠ¸)
                    rank = idx
                    rank_container = link_elem.select_one('div:first-child')
                    if rank_container:
                        medal_img = rank_container.select_one('img[src*="medal"]')
                        if medal_img:
                            src = medal_img.get('src', '')
                            if 'medal_1' in src: rank = 1
                            elif 'medal_2' in src: rank = 2
                            elif 'medal_3' in src: rank = 3
                        else:
                            # 4ìœ„ ì´í•˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            rank_text = rank_container.get_text(strip=True)
                            if rank_text.isdigit():
                                rank = int(rank_text)

                    # 2. ë¸Œëœë“œ ë° ìƒí’ˆëª… ì¶”ì¶œ (h3 íƒœê·¸ ë‚´ spanë“¤)
                    h3_elem = link_elem.select_one('h3')
                    spans = h3_elem.select('span') if h3_elem else []
                    brand = spans[0].get_text(strip=True) if len(spans) > 0 else "Unknown"
                    name = spans[1].get_text(strip=True) if len(spans) > 1 else f"Product {idx}"
                    
                    # 3. ì´ë¯¸ì§€ URL (img íƒœê·¸)
                    img_elem = link_elem.select_one('img.rounded-4') or link_elem.select_one('img[src*="image"]')
                    musinsa_img = img_elem.get('src', '') if img_elem else ""
                    
                    # 4. ê°€ê²© (í˜„ì¬ ë¶„ì„ëœ DOMì—ì„œ í´ë˜ìŠ¤ëª…ì´ ê°€ë³€ì ì´ë¯€ë¡œ ìœ ì—°í•˜ê²Œ ëŒ€ì²˜)
                    price_elem = link_elem.select_one('div.text-14.font-bold') or link_elem.select_one('div[class*="font-bold"]')
                    price = price_elem.get_text(strip=True) if price_elem else "N/A"
                    
                    detail_url = "https://www.hwahae.com" + link_elem.get('href')
                    
                    product = {
                        'rank': rank,
                        'productName': name,
                        'brand': brand,
                        'imageUrl': musinsa_img, # ì•„ë§ˆì¡´ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ì´ë¯¸ì§€
                        'price': price,
                        'buyUrl': f"https://www.amazon.com/s?k={brand}+{name}",
                        'detailUrl': detail_url,
                        'tags': [],
                        'subcategory': 'beauty',
                        'trend': 0,
                        'nikIndex': 0,
                        'culturalContext': ""
                    }
                    products.append(product)
                except Exception as e:
                    print(f"âš ï¸  ì œí’ˆ {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            await browser.close()
            print(f"âœ… ì´ {len(products)}ê°œ ì œí’ˆ ì¶”ì¶œ ì™„ë£Œ")
                    
    except Exception as e:
        print(f"âŒ í™”í•´ ê¸€ë¡œë²Œ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
    return products

async def fetch_hwahae_reviews(url: str, max_reviews: int = 5) -> List[str]:
    """ì œí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ í•œêµ­ì–´ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    reviews = []
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(locale='ko-KR')
            page = await context.new_page()
            
            # ë¦¬ë·° íƒ­ìœ¼ë¡œ ì§ì ‘ ì´ë™ ì‹œë„ ë˜ëŠ” í´ë¦­
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # ë¦¬ë·° ì„¹ì…˜ ë¡œë“œ ëŒ€ê¸°
            await page.evaluate("window.scrollTo(0, 1000)")
            await asyncio.sleep(1)
            
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ë¦¬ë·° í…ìŠ¤íŠ¸ ì…€ë ‰í„° (ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
            review_elems = soup.select('div._review_text_1k2l9_1')[:max_reviews]
            reviews = [r.get_text(strip=True) for r in review_elems]
            
            await browser.close()
    except Exception as e:
        print(f"âš ï¸  ë¦¬ë·° ìˆ˜ì§‘ ì˜¤ë¥˜ ({url}): {e}")
    return reviews
async def calculate_trends(db, category_key: str, current_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì´ì „ ë‚ ì§œ ë­í‚¹ê³¼ ë¹„êµí•˜ì—¬ íŠ¸ë Œë“œ ê³„ì‚°
    
    NOTE: ì´ í•¨ìˆ˜ëŠ” ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ í›„ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤!
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤
        current_products: í˜„ì¬ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´ ë²ˆì—­ ì™„ë£Œëœ ìƒíƒœ)
        
    Returns:
        íŠ¸ë Œë“œê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    from datetime import timedelta
    
    try:
        # ì–´ì œ ë‚ ì§œ (UTC)
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
        doc_id = f"{yesterday}_{firestore_category}"
        
        print(f"\nğŸ“Š íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday}, ì¹´í…Œê³ ë¦¬: {firestore_category})")
        
        # ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            # ì–´ì œ ë°ì´í„° ì—†ìœ¼ë©´ íŠ¸ë Œë“œ 0
            for product in current_products:
                product['trend'] = 0
            return current_products
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        # ì œí’ˆëª…ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ìˆœìœ„ ë³€ë™ ê³„ì‚°
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current_item in current_products:
            current_rank = current_item['rank']
            product_name = current_item['productName']
            brand = current_item.get('brand', '')
            
            # 1ì°¨: ì œí’ˆëª…ìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item.get('productName') == product_name:
                    yesterday_rank = old_item.get('rank')
                    break
            
            # 2ì°¨: ì œí’ˆëª…ì´ ë§¤ì¹­ ì•ˆë˜ë©´ ë¸Œëœë“œ + ìˆœìœ„ ë²”ìœ„ë¡œ ë³´ì¡° ë§¤ì¹­
            if yesterday_rank is None and brand:
                for old_item in yesterday_items:
                    # ë¸Œëœë“œê°€ ê°™ê³  ìˆœìœ„ ì°¨ì´ê°€ Â±3 ì´ë‚´
                    if (old_item.get('brand') == brand and 
                        abs(old_item.get('rank', 999) - current_rank) <= 3):
                        # ì œí’ˆëª… ì¼ë¶€ ìœ ì‚¬ì„± ì²´í¬ (ê°„ë‹¨í•œ ë‹¨ì–´ ë§¤ì¹­)
                        old_name_words = set(old_item.get('productName', '').lower().split())
                        new_name_words = set(product_name.lower().split())
                        common_words = old_name_words & new_name_words
                        if len(common_words) >= 2:  # 2ê°œ ì´ìƒ ë‹¨ì–´ ì¼ì¹˜
                            yesterday_rank = old_item.get('rank')
                            print(f"  ğŸ” ë³´ì¡° ë§¤ì¹­: {product_name[:30]}... (rank {current_rank} â‰ˆ {yesterday_rank})")
                            break
            
            if yesterday_rank:
                # íŠ¸ë Œë“œ = ì–´ì œ ìˆœìœ„ - ì˜¤ëŠ˜ ìˆœìœ„ (ì–‘ìˆ˜ë©´ ìƒìŠ¹)
                trend = yesterday_rank - current_rank
                current_item['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {product_name[:40]}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                # ì‹ ê·œ ì§„ì…
                current_item['trend'] = 0
                trend_changes.append(f"  {product_name[:40]}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ)
        if trend_changes:
            print("ğŸ“ˆ íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
        
        return current_products
        
    except Exception as e:
        print(f"âš ï¸  íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •
        for product in current_products:
            product['trend'] = 0
        return current_products
def auto_romanize_korean(text: str) -> str:
    """
    í•œê¸€ì„ ë¡œë§ˆìë¡œ ìë™ ë³€í™˜
    
    Args:
        text: í•œê¸€ ë˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸
        
    Returns:
        ë¡œë§ˆì ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì´ë¯¸ ì˜ì–´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
    """
    try:
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        has_korean = any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in text)
        
        if has_korean:
            # Transliter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            transliter = Transliter(academic)
            # í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜
            romanized = transliter.translit(text)
            # ê° ë‹¨ì–´ì˜ ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ (Title Case)
            return romanized.title()
        else:
            # ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    except Exception as e:
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        print(f"âš ï¸  Romanization ì˜¤ë¥˜ ({text}): {e}")
        return text


def normalize_product_name(name: str) -> str:
    """
    ì œí’ˆëª…ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    
    Args:
        name: ì›ë³¸ ì œí’ˆëª…
        
    Returns:
        ì •ê·œí™”ëœ ì œí’ˆëª…
    """
    # [ê¸°íš], [ë‹¨í’ˆ], (ì¦ì •) ë“± ì œê±°
    name = re.sub(r'\[.*?\]', '', name)
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì¼ë¶€ë§Œ)
    name = re.sub(r'\([^)]*ê¸°íš[^)]*\)', '', name)
    name = re.sub(r'\([^)]*ì¦ì •[^)]*\)', '', name)
    # +ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì œê±°
    name = re.sub(r'\+.*$', '', name)
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    name = re.sub(r'\s+', ' ', name)
    
    return name.strip()

def translate_brand_names(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë³€í™˜ (ë§¤í•‘ + ìë™ romanization í•˜ì´ë¸Œë¦¬ë“œ)
    
    Args:
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë¸Œëœë“œëª…ì´ ì˜ì–´ë¡œ ë³€í™˜ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ ë¸Œëœë“œëª… ì˜ì–´ ë³€í™˜ ë° ì œí’ˆëª… ì •ê·œí™” ì¤‘...")
    
    new_brands = {}
    
    for product in products:
        korean_brand = product['brand'].strip()
        
        # 1. ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì˜ì–´ ë¸Œëœë“œëª… ì°¾ê¸° (ìš°ì„ ìˆœìœ„)
        if korean_brand in BRAND_NAME_MAPPING:
            product['brand'] = BRAND_NAME_MAPPING[korean_brand]
        else:
            # 2. ìë™ romanization
            romanized = auto_romanize_korean(korean_brand)
            product['brand'] = romanized
            new_brands[korean_brand] = romanized
        
        # ì œí’ˆëª… ì •ê·œí™” (ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°) ë° í•œê¸€ëª… ë³´ì¡´
        original_name = product['productName']
        normalized_name = normalize_product_name(original_name)
        product['productName'] = normalized_name
        product['productNameKo'] = original_name
    
    # ìƒˆë¡œìš´ ë¸Œëœë“œ ë¡œê¹… (ìë™ ë³€í™˜ëœ ë¸Œëœë“œ)
    if new_brands:
        print(f"ğŸ†• ìƒˆë¡œìš´ ë¸Œëœë“œ ìë™ ë³€í™˜ ({len(new_brands)}ê°œ):")
        for korean, english in list(new_brands.items())[:5]:
            print(f"   - {korean} â†’ {english}")
        if len(new_brands) > 5:
            print(f"   ... ì™¸ {len(new_brands) - 5}ê°œ")
    
    print("âœ… ë¸Œëœë“œëª… ë³€í™˜ ì™„ë£Œ")
    
    return products

def calculate_nik_index(hwahae_rank: int, glowpick_rank: int = None, sns_hype_score: float = None) -> float:
    """
    NIK Beauty Index ì‚°ì¶œ (ë©€í‹° ì†ŒìŠ¤ ê°€ì¤‘ì¹˜ ì „ëµ)
    Final Score = (Hwahae_Pts * 0.4) + (Glowpick_Pts * 0.4) + (Viral_Pts * 0.2)
    ì—­ìˆœ ì ìˆ˜ì œ: 1ìœ„ = 100ì , 2ìœ„ = 99ì ...
    """
    # 1. ì—­ìˆœ ì ìˆ˜ ë³€í™˜ (ìµœëŒ€ 100ì  ê¸°ì¤€)
    hwahae_pts = max(0, 101 - hwahae_rank)
    
    # ê¸€ë¡œìš°í”½ ë° SNS ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° í™”í•´ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒê´€ê´€ê³„ ì˜ˆì¸¡ (í´ë°±)
    if glowpick_rank is None:
        # í™”í•´ ìˆœìœ„ì™€ ìœ ì‚¬í•˜ë˜ ì•½ê°„ì˜ ë³€ë™ì„± ë¶€ì—¬
        glowpick_rank = max(1, hwahae_rank + random.randint(-2, 2))
    
    glowpick_pts = max(0, 101 - glowpick_rank)
    
    if sns_hype_score is None:
        # 1-100 scaleë¡œ ë³€í™˜
        sns_hype_score = max(70, hwahae_pts + random.randint(-5, 5))
        sns_hype_score = min(100, sns_hype_score)

    # 2. ê°€ì¤‘ì¹˜ ì ìš©
    final_score = (hwahae_pts * 0.4) + (glowpick_pts * 0.4) + (sns_hype_score * 0.2)
    
    return round(final_score, 1)

# ì´ì „ translate_to_english í•¨ìˆ˜ëŠ” ìœ„ì˜ translate_brand_namesë¡œ ëŒ€ì²´ë¨

async def translate_product_names_batch(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆëª…ì„ ì¼ê´„ ë²ˆì—­ (Batch Processing)
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì œí’ˆëª…ì´ ì˜ì–´ë¡œ ë²ˆì—­ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸŒ Gemini AIë¡œ ì œí’ˆëª… ì¼ê´„ ë²ˆì—­ ì¤‘...")
    
    # ìºì‹œ ë¡œë“œ
    cache = load_cache()
    
    # ë²ˆì—­ì´ í•„ìš”í•œ ì œí’ˆ í•„í„°ë§
    to_translate = []
    success_indices = []
    for i, p in enumerate(products):
        # ProductNameKoê°€ ì—†ìœ¼ë©´ í˜„ì¬ productName(í•œê¸€)ì„ ì €ì¥
        if 'productNameKo' not in p:
            p['productNameKo'] = p['productName']
            
        # ìºì‹œ í‚¤ ì •ê·œí™” (ë¸Œëœë“œ ë° ìƒí’ˆëª…ì˜ ê³µë°± ì œê±° í›„ ì†Œë¬¸ìí™”)
        clean_brand = re.sub(r'\s+', '', p['brand']).lower()
        clean_name = re.sub(r'\s+', '', p['productName']).lower()
        cache_key = f"{clean_brand}_{clean_name}"
        
        if cache_key in cache and 'translatedName' in cache[cache_key]:
            data = cache[cache_key]
            p['productName'] = data['translatedName'] # ì˜ë¬¸ëª…ìœ¼ë¡œ êµì²´
            p['nikIndex'] = data.get('nikIndex', 90.0)
            p['culturalContext'] = data.get('culturalContext', "")
            if 'buyUrl' in data:
                p['buyUrl'] = data['buyUrl']
            print(f"  ğŸ“¦ ìºì‹œ ì‚¬ìš©: {p['productName']}")
        else:
            to_translate.append(p)
            success_indices.append(i)
    
    if not to_translate:
        print("âœ… ëª¨ë“  ì œí’ˆì´ ìºì‹œì— ì¡´ì¬í•©ë‹ˆë‹¤.")
        return products

    # Gemini í˜¸ì¶œ ì œí•œ (Free Tier RPM)
    if not DEV_MODE:
        time.sleep(4)

    # ì œí’ˆëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    product_names = [f"{p['rank']}. {p['productName']}" for p in to_translate]
    
    prompt = f"""
Translate the following Korean beauty product names into English.
Keep brand names as they are (already in English).
Focus on translating the product description/name part accurately.
Use professional beauty industry terminology.

Additionally, for each product, generate:
1. "nikIndex": A proprietary popularity score (0-100) based on K-Rank's algorithm. 
   Consider Hwahae (40%), Glowpick (40%), and SNS/Viral (20%).
2. "culturalContext": This is now the "AI Analyst Note". Generate a professional, catchy English insight (max 2 sentences).
   Format: "AI Analyst Note: [Insight content]".
   Mention its authority (e.g., "Ranked #1 on Hwahae and trending on Glowpick for its [benefit]").
   Explain why it's the "safest choice" or "most effective" based on Korean user data.
3. "imageQuery": A clean English search term to find this product's image.
4. "glowpickRank": Estimated rank on Glowpick (1-50).
5. "snsHypeScore": Estimated viral score (1-100).

Product Names:
{chr(10).join(product_names)}

Response format (JSON):
{{
  "translations": [
    {{
      "rank": 1, 
      "productName": "English Product Name", 
      "nikIndex": 98.7, 
      "culturalContext": "AI Analyst Note: This product is a cult favorite in Korea, consistently ranking #1 on Hwahae for chemical-free hydration. It's the safest choice for sensitive skin types craving the viral glass skin finish.",
      "imageQuery": "Search Term",
      "glowpickRank": 3,
      "snsHypeScore": 95
    }},
    ...
  ]
}}

JSON only. Use professional beauty industry terminology.
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ë° AI ë°ì´í„° ì ìš©
        translated_count = 0
        updated_cache = False
        
        if translations and len(translations.get('translations', [])) > 0:
            for entry in translations.get('translations', []):
                rank = entry.get('rank')
                # í•´ë‹¹ rankë¥¼ ê°€ì§„ ì œí’ˆ ì°¾ê¸°
                for i, p in enumerate(products):
                    if p['rank'] == rank and i in success_indices:
                        product_name_en = entry.get('productName')
                        # AIê°€ ê³„ì‚°í•œ ê°’ì„ ìš°ì„ í•˜ë˜, ì—†ìœ¼ë©´ ë¡œì»¬ ë¡œì§ìœ¼ë¡œ ë³´ê°•
                        nik_index = entry.get('nikIndex')
                        if nik_index is None:
                            nik_index = calculate_nik_index(
                                p['rank'], 
                                entry.get('glowpickRank'), 
                                entry.get('snsHypeScore')
                            )
                        
                        cultural_context = entry.get('culturalContext', "")
                        image_query = entry.get('imageQuery', "")

                        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬: productNameì´ ë¹„ì–´ìˆê±°ë‚˜ "Product" ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”ì¸ ê²½ìš° ìŠ¤í‚µ
                        if not product_name_en or "Product" in product_name_en or "English Product Name" in product_name_en:
                            print(f"  âš ï¸ AI ë²ˆì—­ í’ˆì§ˆ ë‚®ìŒ, ìŠ¤í‚µ: {p['productName']}")
                            continue
                        
                        # ì˜ë¬¸ëª…ìœ¼ë¡œ êµì²´ ë° ì¸ë±ìŠ¤/ì»¨í…ìŠ¤íŠ¸ ì ìš©
                        p['productName'] = product_name_en
                        p['nikIndex'] = nik_index
                        p['culturalContext'] = cultural_context
                        
                        if image_query:
                            p['buyUrl'] = f"https://www.amazon.com/s?k={image_query.replace(' ', '+')}&tag={os.getenv('NEXT_PUBLIC_AMAZON_AFFILIATE_ID', 'nextidealab-20')}"
                        
                        # ìºì‹œ ì €ì¥ - ì •ê·œí™”ëœ í‚¤ ì‚¬ìš©
                        clean_brand = re.sub(r'\s+', '', p['brand']).lower()
                        clean_name = re.sub(r'\s+', '', p.get('productNameKo', p['productName'])).lower()
                        cache_key = f"{clean_brand}_{clean_name}"
                        
                        cache[cache_key] = {
                            'translatedName': p['productName'],
                            'nikIndex': p['nikIndex'],
                            'culturalContext': p['culturalContext'],
                            'buyUrl': p.get('buyUrl', ""),
                            'updatedAt': datetime.now().isoformat()
                        }
                        updated_cache = True
                        translated_count += 1
                        break
        
        if updated_cache:
            save_cache(cache)
            
        print(f"âœ… ì œí’ˆëª… ë²ˆì—­ ì™„ë£Œ ({translated_count}/{len(to_translate)}ê°œ)")
        
    except Exception as e:
        print(f"âš ï¸ Gemini ë²ˆì—­ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ í´ë°±: ìë™ ë¡œë§ˆì ë³€í™˜(Romanization) ì‹œë„")
        # AI ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë¡œë§ˆì ë³€í™˜ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í•œê¸€ ë…¸ì¶œ ë°©ì§€
        for product in products:
            # ì•„ì§ ì˜ë¬¸ëª…ì´ ì•„ë‹Œ ê²½ìš° (í•œê¸€ì´ í¬í•¨ëœ ê²½ìš°)
            if any('\u3131' <= c <= '\u3163' or '\uac00' <= c <= '\ud7a3' for c in product['productName']):
                product['productName'] = auto_romanize_korean(product['productName'])
    
    return products


async def summarize_reviews_batch(model, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ëœ í•œêµ­ì–´ ë¦¬ë·°ë“¤ì„ ì˜ì–´ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."""
    print("\nğŸ“ Gemini AIë¡œ í•œêµ­ì–´ ë¦¬ë·° ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
    
    to_summarize = [p for p in products if p.get('rawReviews')]
    if not to_summarize:
        print("ğŸ’¡ ìš”ì•½í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return products

    # ë¦¬ë·° ë°ì´í„°ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    review_data = []
    for p in to_summarize:
        reviews_str = "\n".join([f"- {r}" for r in p['rawReviews']])
        review_data.append(f"Rank {p['rank']} ({p['brand']} {p['productName']}):\n{reviews_str}")
    
    prompt = f"""
    Summarize the following Korean customer reviews for each beauty product into a single, catchy, and insightful English sentence.
    The summary should explain why people like it or what its main benefit is (e.g., "Loved for its deep hydration and non-sticky finish").
    
    Products and Reviews:
    {chr(10).join(review_data)}
    
    Response format (JSON):
    {{
      "summaries": [
        {{"rank": 1, "insight": "Summary sentence"}},
        ...
      ]
    }}
    
    JSON only.
    """
    
    try:
        response = await model.generate_content_async(prompt)
        result_text = response.text.strip()
        
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        summary_data = json.loads(result_text)
        
        for item in summary_data.get('summaries', []):
            rank = item.get('rank')
            insight = item.get('insight')
            for p in products:
                if p['rank'] == rank:
                    p['culturalContext'] = insight # ê¸°ì¡´ culturalContext í•„ë“œ ì¬í™œìš© (ì¸ì‚¬ì´íŠ¸ë¡œ ì‚¬ìš©)
                    break
        
        print(f"âœ… ë¦¬ë·° ìš”ì•½ ì™„ë£Œ ({len(summary_data.get('summaries', []))}ê°œ)")
        
    except Exception as e:
        print(f"âš ï¸  ë¦¬ë·° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        
    return products

async def generate_tags(model, products: List[Dict[str, Any]], category: str = 'all') -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì œí’ˆë³„ íƒœê·¸ ìë™ ìƒì„±
    
    Args:
        model: Gemini ëª¨ë¸
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        category: ì¹´í…Œê³ ë¦¬
        
    Returns:
        íƒœê·¸ê°€ ì¶”ê°€ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸ·ï¸  Gemini AIë¡œ ì œí’ˆ íƒœê·¸ ìë™ ìƒì„± ì¤‘...")
    
    # ìºì‹œ ë¡œë“œ
    cache = load_cache()
    
    # íƒœê·¸ ìƒì„±ì´ í•„ìš”í•œ ì œí’ˆ í•„í„°ë§
    to_tag = []
    success_indices = [] # products ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤ë¥¼ ì €ì¥
    for i, p in enumerate(products):
        cache_key = f"{p['brand']}_{p['productName']}"
        if cache_key in cache and 'tags' in cache[cache_key] and cache[cache_key]['tags']:
            p['tags'] = cache[cache_key]['tags']
            print(f"  ğŸ·ï¸ ìºì‹œ ì‚¬ìš©: {p['productName']} (Tags: {', '.join(p['tags'])})")
        else:
            to_tag.append(p)
            success_indices.append(i) # ì›ë³¸ products ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ ì €ì¥
            
    if not to_tag:
        print("âœ… ëª¨ë“  ì œí’ˆ íƒœê·¸ê°€ ìºì‹œì— ì¡´ì¬í•©ë‹ˆë‹¤.")
        return products

    # Gemini í˜¸ì¶œ ì œí•œ (Free Tier RPM)
    if not DEV_MODE:
        time.sleep(4)

    # ì œí’ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì˜ì–´ ë²ˆì—­ëœ ì´ë¦„ ì‚¬ìš©)
    product_info = [f"{p['rank']}. {p['brand']} - {p.get('productNameEn', p['productName'])}" for p in to_tag]
    
    prompt = f"""
Analyze each beauty product and generate 2-3 unique, relevant tags based on the product's actual characteristics.

IMPORTANT: Each product must have DIFFERENT tags based on its name and brand.
- Identify product type (mask, serum, cream, sunscreen, toner, cleanser, ampoule, essence, etc.)
- Identify key benefits (hydrating, brightening, anti-aging, pore care, soothing, acne care, firming, etc.)
- Identify special features (vegan, dermatologist-tested, sensitive skin, natural ingredients, etc.)

DO NOT use generic tags like "Korean Beauty" or "Best Seller" for all products.
Each product should have unique tags that describe what it actually is.

Examples:
- "Medicube Collagen Jelly Cream" â†’ ["Anti-Aging", "Firming", "Collagen Boost"]
- "Isntree Hyaluronic Acid Toner" â†’ ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]
- "Mediheal Tea Tree Mask Pack 10" â†’ ["Sheet Mask", "Acne Care", "Tea Tree"]
- "AESTURA Atobarrier 365 Cream 80ml" â†’ ["Barrier Cream", "Sensitive Skin", "Moisturizing"]

Products:
{chr(10).join(product_info)}

Response format (JSON):
{{
  "tags": [
    {{"rank": {to_tag[0]['rank'] if to_tag else 1}, "tags": ["Hydrating Toner", "Hyaluronic Acid", "Moisture"]}},
    ...
  ]
}}

JSON only. Make sure each product has DIFFERENT tags that reflect its actual characteristics.
"""
    
    try:
        response = await model.generate_content_async(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        tag_data = json.loads(result_text)
        
        # ì œí’ˆì— íƒœê·¸ ì ìš©
        updated_cache = False
        tag_count = 0
        if tag_data and len(tag_data.get('tags', [])) > 0:
            for item in tag_data.get('tags', []):
                rank = item.get('rank')
                tags = item.get('tags', [])
                
                for i, p in enumerate(products):
                    if p['rank'] == rank and i in success_indices:
                        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬: íƒœê·¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ìŠ¤í‚µ
                        if not tags or not isinstance(tags, list) or any(not t or "tag" in t.lower() for t in tags):
                            print(f"  âš ï¸ AI íƒœê·¸ í’ˆì§ˆ ë‚®ìŒ, ìŠ¤í‚µ: {p['productName']}")
                            continue

                        p['tags'] = tags
                        
                        # ìºì‹œ ì—…ë°ì´íŠ¸ - í•œê¸€ëª…(productNameKo)ì„ í‚¤ë¡œ ì‚¬ìš©
                        cache_key = f"{p['brand']}_{p.get('productNameKo', p['productName'])}"
                        if cache_key not in cache:
                            cache[cache_key] = {}
                        cache[cache_key]['tags'] = p['tags']
                        updated_cache = True
                        tag_count += 1
                        break
        
        if updated_cache:
            save_cache(cache)
            
        print(f"âœ… íƒœê·¸ ìƒì„± ì™„ë£Œ ({tag_count}/{len(to_tag)}ê°œ)")
        
    except Exception as e:
        print(f"âš ï¸ Gemini íƒœê·¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print(f"ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©: {category}")
        
        # Gemini ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì‚¬ìš©
        default_tags = category_tags.get(category, ['Korean Beauty', 'Trending'])
        for product in products:
            product['tags'] = default_tags.copy()
    
    return products

async def scrape_netflix(media_type: str = 'tv', max_items: int = 10, max_retries: int = 3) -> List[Dict[str, Any]]:
    """
    Netflix Top 10 South Korea TV Shows/Films í¬ë¡¤ë§
    
    Args:
        media_type: 'tv' ë˜ëŠ” 'film'
        max_items: í¬ë¡¤ë§í•  ìµœëŒ€ ì•„ì´í…œ ìˆ˜ (ê¸°ë³¸ 10ê°œ)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ì œí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    products = []
    
    for attempt in range(max_retries):
        try:
            async with async_playwright() as p:
                print(f"ğŸ¬ Netflix Top 10 í¬ë¡¤ë§ ì‹œì‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )
                
                page = await context.new_page()
                
                # Netflix Top 10 URL (tv ë˜ëŠ” films)
                url = f"https://top10.netflix.com/south-korea/{media_type}"
                print(f"ğŸ“„ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                
                await page.goto(url, wait_until='networkidle', timeout=60000)
                
                # í…Œì´ë¸”ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    await page.wait_for_selector("table tbody tr", timeout=30000)
                except:
                    print("âš ï¸ í…Œì´ë¸” ì…€ë ‰í„° ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
                
                await page.wait_for_timeout(3000)  # ì¶”ê°€ ë Œë”ë§ ëŒ€ê¸°
                
                # HTML ê°€ì ¸ì˜¤ê¸°
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # í…Œì´ë¸” í–‰(Row) ì„ íƒ
                rows = soup.select("table tbody tr")[:max_items]
                print(f"âœ… {len(rows)}ê°œ íƒ€ì´í‹€ ë°œê²¬!")
                
                if len(rows) == 0:
                    print(f"âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    await browser.close()
                    if attempt < max_retries - 1:
                        await asyncio.sleep(5)
                        continue
                    else:
                        return products
                
                for i, row in enumerate(rows, 1):
                    try:
                        # ë¸Œë¼ìš°ì € ë¶„ì„ ê¸°ë°˜ ì…€ë ‰í„°
                        rank_el = row.select_one("span.rank")
                        title_el = row.select_one("td.title button")
                        weeks_el = row.select_one("td[data-uia='top10-table-row-weeks']")
                        img_el = row.select_one("td.title img.desktop-only")
                        
                        rank_text = rank_el.get_text(strip=True) if rank_el else str(i)
                        title = title_el.get_text(strip=True) if title_el else f"Unknown Title {i}"
                        weeks = weeks_el.get_text(strip=True) if weeks_el else "1"
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = img_el.get('src', '') if img_el else 'https://assets.nflxext.com/us/ffe/siteui/common/icons/nficon2016.png'
                        
                        # YouTube íŠ¸ë ˆì¼ëŸ¬ ë§í¬ ìƒì„±
                        trailer_query = f"{title} trailer"
                        trailer_link = f"https://www.youtube.com/results?search_query={trailer_query.replace(' ', '+')}"
                        
                        # media_typeì— ë”°ë¼ type ì„¤ì •
                        item_type = 'TV Show' if media_type == 'tv' else 'Film'
                        default_tag = 'K-Drama' if media_type == 'tv' else 'Korean Film'
                        
                        item = {
                            'rank': int(rank_text) if rank_text.isdigit() else i,
                            'titleEn': title,
                            'titleKo': title,  # ì´í›„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸
                            'imageUrl': image_url,
                            'weeksInTop10': weeks,
                            'type': item_type,
                            'trailerLink': trailer_link,
                            'vpnLink': 'https://nordvpn.com/ko/',
                            'tags': [f"{weeks} Weeks in Top 10", default_tag],
                            'trend': 0
                        }
                        
                        products.append(item)
                        print(f"  {rank_text}ìœ„. {title} ({weeks}ì£¼ ì—°ì† Top 10)")
                        
                    except Exception as e:
                        print(f"âš ï¸ {i}ìœ„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                await browser.close()
                print("âœ… Netflix í¬ë¡¤ë§ ì„±ê³µ!")
                break
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            else:
                import traceback
                traceback.print_exc()
    
    return products

async def translate_media_titles(model, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª©(Netflix)ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    """
    print("\nğŸŒ Gemini AIë¡œ ë¯¸ë””ì–´ ì œëª© í•œêµ­ì–´ ë²ˆì—­ ì¤‘...")
    
    # ì œëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    titles = [f"{item['rank']}. {item['titleEn']}" for item in items]
    
    prompt = f"""
Translate the following Netflix TV Show/Film titles into their official Korean titles.
Some are already Korean dramas, so find their original Korean titles (e.g., 'Squid Game' -> 'ì˜¤ì§•ì–´ ê²Œì„').
Exclude rank numbers from the translation.

Titles:
{chr(10).join(titles)}

Response format (JSON):
{{
  "translations": [
    {{"rank": 1, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    {{"rank": 2, "titleKo": "í•œêµ­ì–´ ì œëª©"}},
    ...
  ]
}}

JSON only.
"""
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        
        translations = json.loads(result_text)
        
        # ë²ˆì—­ ì ìš©
        for trans in translations.get('translations', []):
            rank = trans.get('rank')
            title_ko = trans.get('titleKo')
            
            for item in items:
                if item['rank'] == rank:
                    item['titleKo'] = title_ko
                    break
        
        print(f"âœ… ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸  ë¯¸ë””ì–´ ì œëª© ë²ˆì—­ ì˜¤ë¥˜: {e}")
        # ì‹¤íŒ¨ ì‹œ ì˜ì–´ ì œëª©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        for item in items:
            item['titleKo'] = item['titleEn']
            
    return items

def get_google_place_stats(name: str, address: str) -> Dict[str, Any]:
    """
    Google Places API (New)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œì˜ ìƒì„¸ í†µê³„(í‰ì , ë¦¬ë·° ìˆ˜)ë¥¼ ê°€ì ¸ì˜´
    """
    api_key = os.getenv('GOOGLE_PLACES_API_KEY') or os.getenv('GOOGLE_MAPS_API_KEY')
    if not api_key:
        return {}
        
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName,places.rating,places.userRatingCount,places.googleMapsUri,places.photos"
    }
    
    # "ì¥ì†Œëª… + ì£¼ì†Œ(ì¼ë¶€)"ë¡œ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ ë†’ì„
    search_query = f"{name} {address.split()[:2]}" 
    data = {
        "textQuery": search_query,
        "languageCode": "en",
        "maxResultCount": 1
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=10)
        if response.status_code == 200:
            places = response.json().get("places", [])
            if places:
                p = places[0]
                return {
                    "rating": p.get("rating", 0),
                    "userRatingCount": p.get("userRatingCount", 0),
                    "googleMapsUri": p.get("googleMapsUri", ""),
                    "photo_name": p.get("photos", [{}])[0].get("name", "") if p.get("photos") else ""
                }
    except Exception as e:
        print(f"âš ï¸ Google Places API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ({name}): {e}")
        
    return {}

async def scrape_tour_api(max_items: int = 50) -> List[Dict[str, Any]]:
    """
    í•œêµ­ê´€ê´‘ê³µì‚¬ TourAPIë¥¼ í†µí•´ ì¸ê¸° ì—¬í–‰ì§€ ì •ë³´ ìˆ˜ì§‘
    """
    api_key = os.getenv('TOUR_API_KEY')
    if not api_key:
        print("âŒ TOUR_API_KEY not found in environment")
        return []

    # areaBasedList2 ì—”ë“œí¬ì¸íŠ¸ (êµ­ë¬¸ ì„œë¹„ìŠ¤ê°€ ë°ì´í„°ê°€ ê°€ì¥ í’ë¶€í•¨)
    base_url = "https://apis.data.go.kr/B551011/KorService2/areaBasedList2"
    
    # ê³µí†µ íŒŒë¼ë¯¸í„° (arrange=B: ì¸ê¸°ìˆœ)
    common_params = f"&numOfRows={max_items}&pageNo=1&MobileOS=ETC&MobileApp=KRank&_type=json&arrange=B"

    content_types = [12, 14, 15]
    all_places = []
    
    for c_type in content_types:
        try:
            print(f"ğŸŒ TourAPI ìš”ì²­ ì¤‘ (contentTypeId: {c_type})...")
            # serviceKeyë¥¼ ì§ì ‘ í¬í•¨í•œ URL ìƒì„± (Double Encoding ë°©ì§€)
            url = f"{base_url}?serviceKey={api_key}{common_params}&contentTypeId={c_type}"
            
            response = requests.get(url, timeout=15)
            if response.status_code == 200:
                try:
                    data = response.json()
                except Exception:
                    print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨")
                    continue

                if not isinstance(data, dict):
                    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹")
                    continue

                # ì•ˆì „í•˜ê²Œ ì¤‘ì²©ëœ ë°ì´í„° ì¶”ì¶œ
                response_obj = data.get("response", {})
                if not isinstance(response_obj, dict):
                    print(f"âŒ TourAPI ì‘ë‹µ ì˜¤ë¥˜ (responseê°€ dict ì•„ë‹˜)")
                    continue
                
                body = response_obj.get("body", {})
                if not isinstance(body, dict):
                    if "resultCode" in data:
                        print(f"âŒ TourAPI ë¹„ì¦ˆë‹ˆìŠ¤ ì˜¤ë¥˜ ë°œìƒ")
                    else:
                        print(f"âŒ TourAPI ì‘ë‹µ ì˜¤ë¥˜ (bodyê°€ dict ì•„ë‹˜)")
                    continue
                
                items_obj = body.get("items", {})
                items = []
                if isinstance(items_obj, dict):
                    items = items_obj.get("item", [])
                elif isinstance(items_obj, str) and not items_obj:
                    items = []
                
                if not items:
                    print(f"âš ï¸ contentTypeId {c_type}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                if isinstance(items, dict):
                    items = [items]
                
                for item in items:
                    title = item.get("title")
                    if not title: continue
                    
                    place = {
                        "name_ko": title,
                        "name_en": title, # Gemini ë‹¨ê³„ì—ì„œ ë²ˆì—­ë¨
                        "address_ko": item.get("addr1", ""),
                        "location": item.get("addr1", "").split()[0] if item.get("addr1") else "Unknown",
                        "imageUrl": item.get("firstimage") or "https://images.unsplash.com/photo-1544273677-277914bd9466?w=800&fit=crop",
                        "mapx": item.get("mapx"),
                        "mapy": item.get("mapy"),
                        "content_id": item.get("contentid"),
                        "content_type": c_type,
                        "views": "N/A",  # TourAPIì—ì„œ ì§ì ‘ ì œê³µë˜ì§€ ì•ŠìŒ
                        "category": "Culture", # Gemini ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
                    }
                    all_places.append(place)
            else:
                print(f"âŒ TourAPI ìš”ì²­ ì‹¤íŒ¨ (HTTP {response.status_code})")
        except Exception as e:
            print(f"âŒ TourAPI ì˜¤ë¥˜: {e}")

    # í†µí•© í›„ ì¡°íšŒìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•˜ê±°ë‚˜ ì„ì–´ì„œ ìƒìœ„ Nê°œ ì¶”ì¶œ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í•©ì¹¨)
    # ì‹¤ì œë¡œëŠ” êµ­ë¬¸ ì„œë¹„ìŠ¤ì—ì„œ ì¡°íšŒìˆ˜ë¥¼ ê°€ì ¸ì™€ì•¼ ì •í™•í•˜ì§€ë§Œ, 
    # EngServiceì˜ arrange=Bë„ ì–´ëŠ ì •ë„ ì‘ë™í•¨.
    return all_places[:max_items]

async def enrich_place_data(model, places: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Gemini AIë¡œ ì¥ì†Œ ì •ë³´ ê°•í™” (ì˜ë¬¸ ë²ˆì—­, AI Story, Photo Spot, Klook Keyword)
    """
    if not places:
        return []

    print("\nğŸ¤– Gemini AIë¡œ ì¥ì†Œ ì •ë³´ ê°•í™” ì¤‘...")
    
    enriched_places = []
    
    for i, place in enumerate(places, 1):
        prompt = f"""
Analyze this South Korean tourist spot: "{place['name_en']}" (Address: {place['address_ko']})
Generate the following information in JSON format:
1. "name_en": Official or well-known English name.
2. "ai_story": A fascinating 2-line story about its historical or cultural context.
3. "photo_spot": A specific tip for the best "Pro Photo Spot" angle (Instagrammable).
4. "tags": An array of 3 short keywords representing the vibe (e.g., ["Nature", "Hiking", "Autumn"]).
5. "category": One of these three categories: "Culture" (for palaces, temples, history), "Nature" (for mountains, beaches, parks), or "Modern" (for shopping, cafes, urban).
6. "hype_score": A score from 1-100 based on current SNS trend, Naver Map "Saves" level, and global search volume.
7. "klook_keyword": A search keyword for Klook (e.g., "Nami Island Tour").
8. "image_query": A high-quality English search keyword to find a representative photo of this place (e.g., "Gyeongbokgung Palace", "Myeongdong Night").

JSON only.
"""
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            if result_text.startswith('```'):
                result_text = result_text.split('```')[1]
                if result_text.startswith('json'):
                    result_text = result_text[4:]
            
            ai_data = json.loads(result_text)
            
            place["name_en"] = ai_data.get("name_en", place["name_en"])
            place["ai_story"] = ai_data.get("ai_story", "")
            place["photo_spot"] = ai_data.get("photo_spot", "")
            place["tags"] = ai_data.get("tags", ["Must Visit", "South Korea", "Trending"])
            place["category"] = ai_data.get("category", "Culture")
            ai_hype_score = ai_data.get("hype_score", 50)
            # 2. Hybrid Verification with Google Places Data
            google_data = get_google_place_stats(place["name_en"], place["address_ko"])
            
            if google_data:
                g_rating = google_data.get("rating", 0)
                g_reviews = google_data.get("userRatingCount", 0)
                # Google Hype Score = (Rating/5) * log10(Reviews+1) * 20 
                # (ìµœëŒ€ ì ìˆ˜ë¥¼ ì•½ 100ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ì„¤ê³„)
                google_hype = (g_rating / 5.0) * min(5, math.log10(g_reviews + 1)) * 20
                final_hype_score = int((google_hype * 0.7) + (ai_hype_score * 0.3))
                place["google_maps_url"] = google_data.get("googleMapsUri", "")
                print(f"    ğŸ” Google Verified: Rating {g_rating}, Reviews {g_reviews} -> Score: {int(google_hype)}")
            else:
                final_hype_score = ai_hype_score # Fallback to AI estimation
            
            # Commercial Match Bonus
            commercial_keywords = ["Palace", "Nami Island", "DMZ", "Seongsu", "Hannam", "Lotte World", "Everland"]
            commercial_bonus = 20 if any(kw.lower() in place["name_en"].lower() for kw in commercial_keywords) else 0
            
            # Final Score Calculation (Mixed Engine)
            # FinalScore = (TourAPI_Views_Index * 0.4) + (Hybrid_Hype_Score * 0.4) + (Commercial_Bonus * 0.2)
            try:
                base_views = int(place.get("views", 0)) if place.get("views") != "N/A" else 0
            except:
                base_views = 0
            
            # Normalize views index (relative)
            # 10,000 views -> approx 40 points in weighting
            views_weight = min(40, (base_views / 10000.0) * 40)
            
            place["final_score"] = views_weight + (final_hype_score * 0.4) + (commercial_bonus)
            place["hype_score"] = final_hype_score

            # Link & Data Generation
            klook_keyword = ai_data.get("klook_keyword", place["name_en"])
            place["klook_url"] = f"https://www.klook.com/en-US/search?query={klook_keyword.replace(' ', '%20')}&action=search"
            place["creatrip_url"] = f"https://www.creatrip.com/en/search?keyword={place['name_en'].replace(' ', '%20')}"
            
            # Determine priority_platform
            if place["category"] == "Culture":
                place["priority_platform"] = "Creatrip"
                place["booking_url"] = place["creatrip_url"]
            else:
                place["priority_platform"] = "Klook"
                place["booking_url"] = place["klook_url"]

            # Image logic enhancement: Use Google Photo > TourAPI > AI Fallback
            api_key = os.getenv('GOOGLE_PLACES_API_KEY') or os.getenv('GOOGLE_MAPS_API_KEY')
            
            # 1. Check if Google has a real photo
            if google_data and google_data.get("photo_name") and api_key:
                photo_ref = google_data["photo_name"]
                place["imageUrl"] = f"https://places.googleapis.com/v1/{photo_ref}/media?maxWidthPx=1200&maxHeightPx=800&key={api_key}"
                print(f"    ğŸ“¸ Image Source: Google Places (Verified)")
            # 2. If no Google photo, Check if TourAPI is valid
            elif not place.get("imageUrl") or "photo-1544273677-277914bd9466" in place["imageUrl"]:
                # 3. Last Resort: AI Guided category fallback
                print(f"    âš ï¸ Image Source: AI Category Fallback (TourAPI/Google missing)")
                category_id_map = {
                    "Culture": "1544273677-277914bd9466",
                    "Nature": "1538332152395-65715509746e",
                    "Modern": "1538485399081-7191377e8241"
                }
                cid = category_id_map.get(place["category"], "1544273677-277914bd9466")
                place["imageUrl"] = f"https://images.unsplash.com/photo-{cid}?w=800&fit=crop"
            else:
                print(f"    ğŸ–¼ï¸ Image Source: TourAPI (External)")

            place["verified_by_mix"] = True
            enriched_places.append(place)
            print(f"  âœ… {i}. {place['name_en']} ê°•í™” ì™„ë£Œ (Hype: {ai_hype_score}, Bonus: {commercial_bonus})")
        except Exception as e:
            print(f"âš ï¸ {place['name_en']} ê°•í™” ì¤‘ ì˜¤ë¥˜: {e}")
            place["ai_story"] = "Discover the hidden gem of Korea."
            place["photo_spot"] = "Best captured at golden hour."
            place["tags"] = ["Must Visit", "South Korea"]
            place["booking_url"] = f"https://www.klook.com/en-US/search?query={place['name_en'].replace(' ', '%20')}&action=search"
            place["hype_score"] = 50
            place["final_score"] = 50
            place["verified_by_mix"] = False
            enriched_places.append(place)

    # Sort by final_score descending
    enriched_places.sort(key=lambda x: x.get("final_score", 0), reverse=True)
    
    # Re-assign rank
    for i, place in enumerate(enriched_places, 1):
        place["rank"] = i
        
    return enriched_places

async def calculate_place_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Place ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_place"
        
        print(f"\nğŸ“Š Place íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday})")
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            for item in current_items:
                item['trend'] = 0
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        
        for current in current_items:
            # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if old_item.get('name_en') == current.get('name_en'):
                    yesterday_rank = old_item.get('rank')
                    break
            
            if yesterday_rank:
                current['trend'] = yesterday_rank - current['rank']
            else:
                current['trend'] = 0
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ Place íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        for item in current_items:
            item['trend'] = 0
        return current_items

async def calculate_media_trends(db, current_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë¯¸ë””ì–´ ë­í‚¹ íŠ¸ë Œë“œ ê³„ì‚°"""
    from datetime import timedelta
    
    try:
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).strftime('%Y-%m-%d')
        doc_id = f"{yesterday}_media"
        
        print(f"\nğŸ“Š Media íŠ¸ë Œë“œ ê³„ì‚° ì¤‘... (ì–´ì œ: {yesterday})")
        
        doc_ref = db.collection('daily_rankings').document(doc_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âš ï¸  ì–´ì œ Media ë°ì´í„° ì—†ìŒ (ë¬¸ì„œ ID: {doc_id})")
            print("ğŸ’¡ ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì–´ì œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ 0ìœ¼ë¡œ ì„¤ì •")
            for item in current_items:
                item['trend'] = 0
            return current_items
        
        yesterday_items = doc.to_dict().get('items', [])
        print(f"âœ… ì–´ì œ Media ë°ì´í„° {len(yesterday_items)}ê°œ ë°œê²¬")
        
        trend_changes = []
        matched_count = 0
        new_count = 0
        
        for current in current_items:
            title_en = current.get('titleEn', '')
            title_ko = current.get('titleKo', '')
            current_rank = current['rank']
            
            # ì˜ì–´ ì œëª© ë˜ëŠ” í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            yesterday_rank = None
            for old_item in yesterday_items:
                if (old_item.get('titleEn') == title_en or 
                    old_item.get('titleKo') == title_ko):
                    yesterday_rank = old_item.get('rank')
                    break
            
            if yesterday_rank:
                trend = yesterday_rank - current_rank
                current['trend'] = trend
                trend_symbol = '+' if trend > 0 else ''
                trend_changes.append(f"  {title_ko or title_en}: {yesterday_rank}ìœ„ â†’ {current_rank}ìœ„ (ë³€ë™: {trend_symbol}{trend})")
                matched_count += 1
            else:
                current['trend'] = 0
                trend_changes.append(f"  {title_ko or title_en}: ì‹ ê·œ ì§„ì… (ë³€ë™: NEW)")
                new_count += 1
        
        # íŠ¸ë Œë“œ ë³€í™” ë¡œê·¸ ì¶œë ¥
        if trend_changes:
            print("ğŸ“ˆ Media íŠ¸ë Œë“œ ë³€í™”:")
            for change in trend_changes[:5]:
                print(change)
            if len(trend_changes) > 5:
                print(f"   ... ì™¸ {len(trend_changes) - 5}ê°œ")
        
        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼: ê¸°ì¡´ {matched_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
                
        return current_items
    except Exception as e:
        print(f"âš ï¸ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        for item in current_items:
            item['trend'] = 0
        return current_items


def save_to_firebase(db, category_key: str, products: List[Dict[str, Any]]):
    """
    Firebase Firestoreì— ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì €ì¥
    
    Args:
        db: Firestore í´ë¼ì´ì–¸íŠ¸
        category_key: ì¹´í…Œê³ ë¦¬ í‚¤ (ì˜ˆ: 'all', 'skincare', 'suncare')
        products: ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ’¾ Firebaseì— {category_key} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì¤‘...")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ (UTC)
    today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
    
    # Firestore ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
    firestore_category = CATEGORY_MAPPING[category_key]['firestore_category']
    
    # ë¬¸ì„œ ID: {ë‚ ì§œ}_{ì¹´í…Œê³ ë¦¬}
    doc_id = f"{today}_{firestore_category}"
    doc_ref = db.collection('daily_rankings').document(doc_id)
    
    # ë°ì´í„° êµ¬ì¡°
    data = {
        'date': today,
        'category': firestore_category,
        'items': products,
        'updatedAt': firestore.SERVER_TIMESTAMP
    }
    
    # ì €ì¥
    if DEV_MODE and not os.getenv('FORCE_SAVE', 'false').lower() == 'true':
        print(f"ğŸ§ª  [DEV_MODE] Firebase ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤. (ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°)")
        
        # JSON ì§ë ¬í™”ê°€ ì•ˆ ë˜ëŠ” SERVER_TIMESTAMP ì²˜ë¦¬
        preview_data = data.copy()
        preview_data['updatedAt'] = "SERVER_TIMESTAMP"
        print(json.dumps(preview_data, ensure_ascii=False, indent=2)[:1000] + "...")
        return

    doc_ref.set(data)
    
    print(f"âœ… {len(products)}ê°œ ì œí’ˆì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
    print(f"ğŸ“ ì»¬ë ‰ì…˜: daily_rankings")
    print(f"ğŸ“„ ë¬¸ì„œ ID: {doc_id}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - Mediaì™€ Place ë°ì´í„°ë§Œ ìë™ í¬ë¡¤ë§"""
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· K-Rank Scraper - Media & Place")
    print("=" * 60)
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì í™•ì¸
    run_mode = sys.argv[1] if len(sys.argv) > 1 else "all"  # "media", "place", "all"
    
    # BeautyëŠ” ì´ì œ import_editorial_ranking.pyë¥¼ í†µí•´ ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬ë¨
    if run_mode == "beauty":
        print("\n" + "=" * 60)
        print("âš ï¸  Beauty ì¹´í…Œê³ ë¦¬ëŠ” ë” ì´ìƒ ìë™ í¬ë¡¤ë§ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ğŸ“ ëŒ€ì‹  scripts/import_editorial_ranking.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        print("=" * 60)
        sys.exit(0)
    
    try:
        # 1. Firebase ì´ˆê¸°í™”
        print("\nğŸ“± Firebase ì´ˆê¸°í™” ì¤‘...")
        db = initialize_firebase()
        print("âœ… Firebase ì—°ê²° ì™„ë£Œ")
        
        # 2. Gemini ì´ˆê¸°í™”
        print("\nğŸ¤– Gemini AI ì´ˆê¸°í™” ì¤‘...")
        model = initialize_gemini()
        print("âœ… Gemini AI ì—°ê²° ì™„ë£Œ")
        
        total_products = 0
        
        # 4. Media ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["media", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ¬ MEDIA ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (Netflix)")
            print("=" * 60)
            
            all_media_items = []
            
            # Netflix TV Shows Top 10 í¬ë¡¤ë§
            print("\nğŸ“º Netflix TV Shows í¬ë¡¤ë§ ì¤‘...")
            actual_limit = DEV_LIMIT if DEV_MODE else 10
            tv_items = await scrape_netflix(media_type='tv', max_items=actual_limit)
            if tv_items:
                all_media_items.extend(tv_items)
                print(f"âœ… TV Shows {len(tv_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ TV Shows ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # Netflix Films Top 10 í¬ë¡¤ë§
            print("\nğŸ¬ Netflix Films í¬ë¡¤ë§ ì¤‘...")
            film_items = await scrape_netflix(media_type='films', max_items=actual_limit)
            if film_items:
                all_media_items.extend(film_items)
                print(f"âœ… Films {len(film_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("âš ï¸ Films ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            if all_media_items:
                # í•œêµ­ì–´ ì œëª© ë²ˆì—­ (ë¨¼ì € ì‹¤í–‰)
                all_media_items = await translate_media_titles(model, all_media_items)
                
                # íŠ¸ë Œë“œ ê³„ì‚° (ë²ˆì—­ í›„ ì‹¤í–‰í•˜ì—¬ ì˜ì–´/í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë§¤ì¹­)
                all_media_items = await calculate_media_trends(db, all_media_items)
                
                # Media ì €ì¥ ë¡œì§
                today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
                doc_id = f"{today}_media"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'media',
                    'items': all_media_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                if WRITE_TO_FIRESTORE:
                    doc_ref.set(data)
                    print(f"âœ… {len(all_media_items)}ê°œ íƒ€ì´í‹€ì„ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                else:
                    print(f"ğŸ§ª [DEV_MODE] Firebase Media ì €ì¥ ìŠ¤í‚µ ({len(all_media_items)}ê°œ)")
                print(f"   - TV Shows: {len(tv_items)}ê°œ")
                print(f"   - Films: {len(film_items)}ê°œ")
                total_products += len(all_media_items)
            else:
                print("âš ï¸ Netflixì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 5. Place ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        if run_mode in ["place", "all"]:
            print("\n" + "=" * 60)
            print("ğŸ—ºï¸  PLACE ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (TourAPI)")
            print("=" * 60)
            
            # TourAPI ìˆ˜ì§‘
            place_items = await scrape_tour_api(max_items=50)
            
            if place_items:
                # Gemini AI ê°•í™”
                place_items = await enrich_place_data(model, place_items)
                
                # íŠ¸ë Œë“œ ê³„ì‚°
                place_items = await calculate_place_trends(db, place_items)
                
                # ì €ì¥
                today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
                doc_id = f"{today}_place"
                doc_ref = db.collection('daily_rankings').document(doc_id)
                
                data = {
                    'date': today,
                    'category': 'place',
                    'items': place_items,
                    'updatedAt': firestore.SERVER_TIMESTAMP
                }
                
                if WRITE_TO_FIRESTORE:
                    doc_ref.set(data)
                    print(f"âœ… {len(place_items)}ê°œ ëª…ì†Œë¥¼ {doc_id} ë¬¸ì„œì— ì €ì¥ ì™„ë£Œ")
                else:
                    print(f"ğŸ§ª [DEV_MODE] Firebase Place ì €ì¥ ìŠ¤í‚µ ({len(place_items)}ê°œ)")
                total_products += len(place_items)
            else:
                print("âš ï¸ TourAPIì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"  - ì´ ì•„ì´í…œ ìˆ˜: {total_products}ê°œ")
        print(f"  - ì‹¤í–‰ ëª¨ë“œ: {run_mode.upper()}")

        # ë°ì´í„° ê²€ì¦: ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        if total_products == 0:
            print(f"\nâŒ [CRITICAL] {run_mode.upper()} ëª¨ë“œì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
            print("ğŸ’¡ í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì ‘ê·¼ì´ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # ì‚¬ìš©ë²•:
    # python scraper.py           # Mediaì™€ Place ëª¨ë‘ ì‹¤í–‰
    # python scraper.py media     # Mediaë§Œ ì‹¤í–‰
    # python scraper.py place     # Placeë§Œ ì‹¤í–‰
    # 
    # Beauty ë°ì´í„°ëŠ” scripts/import_editorial_ranking.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
    asyncio.run(main())
