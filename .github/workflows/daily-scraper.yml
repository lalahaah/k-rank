name: Daily Scraper (Beauty)

on:
  # 매일 한국 시간 오전 9시 (UTC 0시) 실행
  schedule:
    - cron: '0 0 * * *'
  
  # 수동 실행 가능
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install playwright python-dotenv firebase-admin google-generativeai beautifulsoup4 requests hangul-romanize
          playwright install chromium
          playwright install-deps chromium
      
      - name: Create Firebase service account key
        run: |
          echo '${{ secrets.FIREBASE_SERVICE_ACCOUNT }}' > serviceAccountKey.json
      
      - name: Create .env file
        run: |
          echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" > .env
          echo "WEBSCRAPING_AI_API_KEY=${{ secrets.WEBSCRAPING_AI_API_KEY }}" >> .env
      
      - name: Run Beauty Scraper
        run: |
          python3 scripts/scraper.py beauty
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          WEBSCRAPING_AI_API_KEY: ${{ secrets.WEBSCRAPING_AI_API_KEY }}
      
      - name: Cleanup
        if: always()
        run: |
          rm -f serviceAccountKey.json
          rm -f .env
